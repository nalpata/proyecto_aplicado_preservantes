"""
run_pipeline.py
Script para ejecutar el pipeline completo sin interfaz gr√°fica.
√ötil para procesamiento batch.
"""

import sys
from pathlib import Path

# Agregar src al path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from data_ingestion import PDFIngestion
from text_extraction import TextExtractor
from chunking import DocumentChunker
from metadata_extraction import MetadataExtractor
from vector_db import VectorDatabase
from retriever import SimpleRetriever
from benchmark import RAGBenchmark


def main():
    """Ejecuta el pipeline completo."""
    
    print("=" * 60)
    print("üß¨ PRESERV-RAG - PIPELINE BASELINE (HITO 1)")
    print("=" * 60)
    
    # Configuraci√≥n
    pdf_folder = "data/pdfs"
    db_path = "data/chroma_db"
    chunk_size = 500
    overlap = 50
    
    # 1. INGESTA
    print("\n1Ô∏è‚É£  INGESTA DE PDFs")
    print("-" * 60)
    try:
        ingestion = PDFIngestion(pdf_folder=pdf_folder)
        documents = ingestion.load_pdfs()
        ingest_stats = ingestion.get_stats()
        
        print(f"‚úì {ingest_stats['total_pdfs']} PDFs cargados")
        print(f"  - Total p√°ginas: {ingest_stats['total_pages']}")
        print(f"  - Total caracteres: {ingest_stats['total_characters']:,}")
    except Exception as e:
        print(f"‚úó Error en ingesta: {str(e)}")
        return
    
    # 2. EXTRACCI√ìN, LIMPIEZA Y FILTRADO DE TEXTO
    print("\n2Ô∏è‚É£  EXTRACCI√ìN, LIMPIEZA Y FILTRADO DE TEXTO")
    print("-" * 60)
    try:
        # Usar configuraci√≥n por defecto: filtrar todo
        extractor = TextExtractor(
            filter_sections=True,
            remove_references=True,
            remove_acknowledgments=True,
            remove_appendix=True,
            remove_tables=True,
            remove_headers_footers=True
        )
        cleaned_docs = extractor.process_documents(documents)
        extract_stats = extractor.get_stats(cleaned_docs)

        print("‚úì Texto procesado (limpieza + filtrado)")
        print(f"  - Reducci√≥n de caracteres: {extract_stats['reduction_percentage']}%")
        print(f"  - Caracteres originales: {extract_stats['total_original_chars']:,}")
        print(f"  - Caracteres finales: {extract_stats['total_cleaned_chars']:,}")

        if extract_stats.get('sections_filtered', False):
            print(f"\n  üìù Secciones filtradas:")
            print(f"    - Referencias: {extract_stats['total_references_removed']}")
            print(f"    - Tablas: {extract_stats['total_tables_removed']}")
            print(f"    - Agradecimientos: {extract_stats['total_acknowledgments_removed']}")
            print(f"    - Ap√©ndices: {extract_stats['total_appendix_removed']}")
            print(f"    - Headers/Footers: {extract_stats['total_headers_footers_removed']}")
    except Exception as e:
        print(f"‚úó Error en extracci√≥n/filtrado: {str(e)}")
        return
    
    # 3. CHUNKING SEM√ÅNTICO
    print("\n3Ô∏è‚É£  DIVISI√ìN EN CHUNKS (SEM√ÅNTICO)")
    print("-" * 60)
    try:
        chunker = DocumentChunker(chunk_size=chunk_size, overlap=overlap, min_chunk_size=100)
        chunks = chunker.chunk_documents(cleaned_docs)
        chunking_stats = chunker.get_stats(chunks)

        print("‚úì Documentos divididos en chunks sem√°nticos")
        print(f"  - Total chunks: {chunking_stats['total_chunks']}")
        print(f"  - Documentos √∫nicos: {chunking_stats['unique_documents']}")
        print(f"  - Tama√±o promedio: {chunking_stats['avg_chunk_size']} caracteres")
        print(f"  - Tama√±o m√≠n/m√°x: {chunking_stats['min_chunk_size']}/{chunking_stats['max_chunk_size']}")
        print(f"  - Chunks por documento: {chunking_stats['chunks_per_document']}")
        print(f"  - P√°rrafos por chunk: {chunking_stats.get('avg_paragraphs_per_chunk', 'N/A')}")
    except Exception as e:
        print(f"‚úó Error en chunking: {str(e)}")
        import traceback
        traceback.print_exc()
        return
    
    # 4. EXTRACCI√ìN DE METADATA
    print("\n4Ô∏è‚É£  EXTRACCI√ìN DE METADATA")
    print("-" * 60)
    try:
        metadata_extractor = MetadataExtractor()
        chunks_with_metadata = metadata_extractor.process_chunks(chunks)
        metadata_stats = metadata_extractor.get_stats(chunks_with_metadata)
        
        print("‚úì Metadata extra√≠da de chunks")
        print(f"  - Chunks con pH: {metadata_stats['chunks_with_ph']}")
        print(f"  - Chunks con aW: {metadata_stats['chunks_with_aw']}")
        print(f"  - Chunks con microorganismos: {metadata_stats['chunks_with_microorganisms']}")
        print(f"  - Chunks con conservantes: {metadata_stats['chunks_with_conservants']}")
        print(f"  - Cobertura de metadata: {metadata_stats['metadata_coverage_pct']}%")
    except Exception as e:
        print(f"‚úó Error en extracci√≥n de metadata: {str(e)}")
        return
    
    # 5. VECTORIZACI√ìN
    print("\n5Ô∏è‚É£  VECTORIZACI√ìN E INDEXACI√ìN")
    print("-" * 60)
    try:
        vdb = VectorDatabase(db_path=db_path, model_name="all-MiniLM-L6-v2")
        vdb.add_chunks(chunks_with_metadata)
        vdb_stats = vdb.get_collection_stats()
        
        print("‚úì Base vectorial creada")
        print(f"  - Total chunks indexados: {vdb_stats['total_chunks']}")
        print(f"  - Modelo usado: {vdb_stats['model_used']}")
        print(f"  - Ruta BD: {vdb_stats['db_path']}")
    except Exception as e:
        print(f"‚úó Error en vectorizaci√≥n: {str(e)}")
        return
    
    # 6. B√öSQUEDA DE PRUEBA
    print("\n6Ô∏è‚É£  B√öSQUEDA DE PRUEBA")
    print("-" * 60)
    try:
        retriever = SimpleRetriever(vdb)
        
        test_queries = [
            "Benzoato alternativa natural pH 4.2 levaduras",
            "Sorbato extracto plantas conservante",
            "Nisina microorganismo inhibici√≥n",
        ]
        
        for i, query in enumerate(test_queries, 1):
            print(f"\nQuery {i}: {query}")
            results = retriever.retrieve(query, n_results=3)
            
            if results:
                for j, result in enumerate(results, 1):
                    print(f"  {j}. [Similitud: {result['similarity_score']:.4f}] {result['content'][:100]}...")
            else:
                print("  No se encontraron resultados")
    except Exception as e:
        print(f"‚úó Error en b√∫squeda: {str(e)}")
        return
    
    # 7. RESUMEN FINAL
    print("\n" + "=" * 60)
    print("‚úÖ PIPELINE COMPLETADO EXITOSAMENTE")
    print("=" * 60)
    print("\nüìä RESUMEN DEL PIPELINE:")
    print(f"  - PDFs procesados: {ingest_stats['total_pdfs']}")
    print(f"  - Secciones filtradas: {extract_stats.get('total_sections_removed', 0)}")
    print(f"  - Chunks creados: {chunking_stats['total_chunks']}")
    print(f"  - Chunks indexados: {vdb_stats['total_chunks']}")
    print(f"  - Metadata extra√≠da: {metadata_stats['metadata_coverage_pct']}%")
    print(f"\nüÜï MEJORAS IMPLEMENTADAS:")
    print("  ‚úì Filtrado de secciones de bajo valor (referencias, tablas, etc.)")
    print("  ‚úì Chunking sem√°ntico (respeta p√°rrafos y oraciones)")
    print("  ‚úì Overlap inteligente entre chunks")
    print("  ‚úì No corta en abreviaciones cient√≠ficas")
    print(f"\nüéØ Para usar la interfaz gr√°fica:")
    print("  streamlit run streamlit_app.py")
    print(f"\nüìÅ Archivos generados:")
    print(f"  - Base vectorial: {db_path}")


if __name__ == "__main__":
    main()
