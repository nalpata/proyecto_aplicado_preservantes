{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nalpata/proyecto_aplicado_preservantes/blob/main/01_naive_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ab4016b",
      "metadata": {
        "id": "9ab4016b"
      },
      "source": [
        "# Build a RAG agent with LangChain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"langchain>=0.2.0\" \"langchain-community\" \"langchain-core\"\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2wDK0RWXPaR",
        "outputId": "56747435-8f4c-43ad-80cd-71643b561c45"
      },
      "id": "E2wDK0RWXPaR",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain>=0.2.0 in /usr/local/lib/python3.12/dist-packages (1.0.8)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.2.0) (1.0.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.2.0) (2.11.10)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.43)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=0.2.0) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=0.2.0) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=0.2.0) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=0.2.0) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.2.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.2.0) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain>=0.2.0) (1.12.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a9754c3",
      "metadata": {
        "id": "2a9754c3"
      },
      "source": [
        "## Overview\n",
        "\n",
        "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\n",
        "\n",
        "This class we will learn how to build a simple Q&A application over an unstructured text data source.\n",
        "\n",
        "We will demonstrate:\n",
        "- A RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\n",
        "- A two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54da7546",
      "metadata": {
        "id": "54da7546"
      },
      "source": [
        "## Concepts\n",
        "We will cover the following concepts:\n",
        "- **Indexing**: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\n",
        "- **Retrieval** and **generation**: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
        "\n",
        "Once we‚Äôve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community langchain-text-splitters langchain-huggingface \"langchain[openai]\" pypdf\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ua_Xso4pAu-j",
        "outputId": "72aa00bb-f2f2-4dc3-c4a6-8a2f8fdca46f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ua_Xso4pAu-j",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.0.8)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.4.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.1.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.43)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.36.0)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.1)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (from langchain[openai]) (1.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (25.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.6->langchain) (1.33)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai->langchain[openai]) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai->langchain[openai]) (0.12.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.6->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai->langchain[openai]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai->langchain[openai]) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai->langchain[openai]) (1.3.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai->langchain[openai]) (2024.11.6)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-mjb2m69BEus"
      },
      "id": "-mjb2m69BEus"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4c39d395",
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "id": "4c39d395",
        "outputId": "42c3c958-444c-4457-a55b-7a9df6715e97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30644e94",
      "metadata": {
        "id": "30644e94"
      },
      "source": [
        "# Components\n",
        "We will need to select three components from LangChain‚Äôs suite of integrations.\n",
        "1. Select a chat model:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0\n",
        ")\n"
      ],
      "metadata": {
        "id": "TU2w8LZdkDaY"
      },
      "id": "TU2w8LZdkDaY",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")\n"
      ],
      "metadata": {
        "id": "-lxlTzLHkJqH"
      },
      "id": "-lxlTzLHkJqH",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "64313d3f",
      "metadata": {
        "id": "64313d3f"
      },
      "source": [
        "2. Select an embeddings model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8fa973f8",
      "metadata": {
        "id": "8fa973f8"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ee66bca",
      "metadata": {
        "id": "4ee66bca"
      },
      "source": [
        "3. Select a vector store:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "vector_store = InMemoryVectorStore(embeddings)\n"
      ],
      "metadata": {
        "id": "ySbNEbnLFP7G"
      },
      "id": "ySbNEbnLFP7G",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VECTOR STORE SIMPLE PARA HITO 1\n",
        "\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "# Creamos el vector store a partir del modelo de embeddings\n",
        "vector_store = InMemoryVectorStore(embeddings)\n",
        "\n",
        "print(\"Vector store en memoria inicializado correctamente.\")\n"
      ],
      "metadata": {
        "id": "p8L4e5A4GpKz",
        "outputId": "761fa36b-0253-4727-9725-62b9d3af8fc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "p8L4e5A4GpKz",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store en memoria inicializado correctamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1dadfba",
      "metadata": {
        "id": "c1dadfba"
      },
      "source": [
        "# 1. Indexing\n",
        "\n",
        "Indexing commonly works as follows:\n",
        "1. **Load**: First we need to load our data. This is done with Document Loaders.\n",
        "2. **Split**: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won‚Äôt fit in a model‚Äôs finite context window.\n",
        "3. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4030fd89",
      "metadata": {
        "id": "4030fd89"
      },
      "source": [
        "## Loading documents\n",
        "We need to first load the blog post contents. We can use `DocumentLoaders` for this, which are objects that load in data from a source and return a list of `Document` objects.\n",
        "In this case we‚Äôll use the `WebBaseLoader`, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text.\n",
        "\n",
        "We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see [BeautifulSoup docs](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup)).\n",
        "\n",
        "In this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or ‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/nalpata/proyecto_aplicado_preservantes.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fkHpjnSSN1p",
        "outputId": "f9a2fe40-a1ae-407d-87c7-cb8afc3dafbd"
      },
      "id": "6fkHpjnSSN1p",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'proyecto_aplicado_preservantes' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/proyecto_aplicado_preservantes\n",
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnhT91UrSU-a",
        "outputId": "56e2d84b-88eb-4cac-ed0d-348bc5c3d5bf"
      },
      "id": "BnhT91UrSU-a",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/proyecto_aplicado_preservantes\n",
            " CHECKLIST_INSTALACION.md   notebooks\t\t   run_pipeline.py\n",
            " data\t\t\t   'Pauta proyecto.pdf'    SOLUCION_INSTALACION.md\n",
            " DIAGRAMAS_HITO_1.md\t    PROBLEMAS_COMUNES.md   src\n",
            " examples.py\t\t    QUICK_START.md\t   streamlit_app.py\n",
            " FIX_CHROMADB.md\t    README.md\t\t   test_improvements.py\n",
            " install_dependencies.sh    requirements.txt\t   TESTING_LOCAL.md\n",
            " install_fix.sh\t\t    RESUMEN_FINAL.md\t   test_single_pdf.py\n",
            " INSTRUCCIONES_PRUEBA.md    RESUMEN_HITO_1.md\n",
            " MEJORAS_CHUNKING.md\t    RESUMEN_PROBLEMAS.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Cargando documentos PDF del dominio de preservantes\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "# Ruta PDFs\n",
        "pdf_path = \"data/pdfs\"\n",
        "\n",
        "loader = PyPDFDirectoryLoader(pdf_path)\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\" Documentos cargados: {len(docs)}\")\n",
        "print(f\" Primer extracto (500 caracteres):\\n\\n{docs[0].page_content[:500]}\")\n"
      ],
      "metadata": {
        "id": "fDHnPTvhHlhh",
        "outputId": "0f00d449-d55f-4507-ae20-08bc7d630a8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "id": "fDHnPTvhHlhh",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Documentos cargados: 475\n",
            " Primer extracto (500 caracteres):\n",
            "\n",
            "antibiotics \n",
            "Review\n",
            "Food Safety through Natural Antimicrobials\n",
            "Emiliano J. Quinto 1, *\n",
            " , Irma Caro 1, Luz H. Villalobos-Delgado 2, Javier Mateo 3\n",
            " ,\n",
            "Beatriz De-Mateo-Silleras 1 and Mar√≠a P . Redondo-Del-R√≠o 1\n",
            "1 Department of Nutrition and Food Science, Faculty of Medicine, University of Valladolid, 47005 Valladolid,\n",
            "Spain; irma.caro@uva.es (I.C.); bdemateo@yahoo.com (B.D.-M.-S.); pazr@ped.uva.es (M.P .R.-D.-R.)\n",
            "2 Institute of Agroindustry, Technological University of the Mixteca, Huajuapan de L\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c86a4fe",
      "metadata": {
        "id": "2c86a4fe"
      },
      "source": [
        "## Splitting documents\n",
        "\n",
        "Our loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
        "\n",
        "To handle this we‚Äôll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\n",
        "\n",
        "As in the semantic search tutorial, we use a `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Chunking pensado para art√≠culos cient√≠ficos largos\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1500,   # tama√±o del trozo\n",
        "    chunk_overlap=200, # solapamiento para no cortar ideas a la mitad\n",
        ")\n",
        "\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"üîπ Total de chunks: {len(all_splits)}\")\n",
        "print(\"üîπ Ejemplo de chunk:\\n\")\n",
        "print(all_splits[0].page_content[:500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZTOgTWglTHoj",
        "outputId": "da698a96-377d-4e29-e8ce-16bf74aa7baf"
      },
      "id": "ZTOgTWglTHoj",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Total de chunks: 1429\n",
            "üîπ Ejemplo de chunk:\n",
            "\n",
            "antibiotics \n",
            "Review\n",
            "Food Safety through Natural Antimicrobials\n",
            "Emiliano J. Quinto 1, *\n",
            " , Irma Caro 1, Luz H. Villalobos-Delgado 2, Javier Mateo 3\n",
            " ,\n",
            "Beatriz De-Mateo-Silleras 1 and Mar√≠a P . Redondo-Del-R√≠o 1\n",
            "1 Department of Nutrition and Food Science, Faculty of Medicine, University of Valladolid, 47005 Valladolid,\n",
            "Spain; irma.caro@uva.es (I.C.); bdemateo@yahoo.com (B.D.-M.-S.); pazr@ped.uva.es (M.P .R.-D.-R.)\n",
            "2 Institute of Agroindustry, Technological University of the Mixteca, Huajuapan de L\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "C2Ird7i2UVI5"
      },
      "id": "C2Ird7i2UVI5",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "# Crear vector store en memoria\n",
        "vector_store = InMemoryVectorStore(embeddings)\n",
        "\n",
        "# Indexar todos los chunks\n",
        "vector_store.add_documents(all_splits)\n",
        "\n",
        "print(\"üì¶ Chunks indexados en el vector store.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFdo1sCvUjwE",
        "outputId": "df78f84b-1ebf-43a9-fa1e-110949c8a9cd"
      },
      "id": "ZFdo1sCvUjwE",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Chunks indexados en el vector store.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d87fbad4",
      "metadata": {
        "id": "d87fbad4"
      },
      "source": [
        "## Storing documents\n",
        "Now we need to index our text chunks so that we can search over them at runtime.\n",
        "\n",
        "Our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\n",
        "\n",
        "We can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "17e035bd",
      "metadata": {
        "id": "17e035bd",
        "outputId": "d4198c40-cec9-406d-adda-5360dab2ce81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a0b7d67d-0ffe-4d08-b5b1-f69c52762232', '4283909d-7081-45be-9022-841b0ab1335e', 'c719735f-e37f-441b-bb53-f23e565552e3']\n"
          ]
        }
      ],
      "source": [
        "document_ids = vector_store.add_documents(documents=all_splits)\n",
        "\n",
        "print(document_ids[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7c05dc3",
      "metadata": {
        "id": "d7c05dc3"
      },
      "source": [
        "# 2. Retrieval and Generation\n",
        "RAG applications commonly work as follows:\n",
        "1. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
        "2. **Generate**: A model produces an answer using a prompt that includes both the question with the retrieved data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a621f1bc",
      "metadata": {
        "id": "a621f1bc"
      },
      "source": [
        "Now let‚Äôs write the actual application logic.\n",
        "\n",
        "We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\n",
        "\n",
        "We will demonstrate:\n",
        "- A RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\n",
        "- A two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4bdf948",
      "metadata": {
        "id": "b4bdf948"
      },
      "source": [
        "## RAG agents\n",
        "One formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "84118645",
      "metadata": {
        "id": "84118645"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool(response_format=\"content_and_artifact\")\n",
        "def retrieve_context(query: str):\n",
        "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
        "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
        "    serialized = \"\\n\\n\".join(\n",
        "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
        "        for doc in retrieved_docs\n",
        "    )\n",
        "    return serialized, retrieved_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3167d869",
      "metadata": {
        "id": "3167d869"
      },
      "source": [
        "Here we use the `@[tool decorator][tool]` to configure the tool to attach raw documents as artifacts to each `ToolMessage`.\n",
        "\n",
        "This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\n",
        "\n",
        "Given our tool, we can construct the agent:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80cd1ea9",
      "metadata": {
        "id": "80cd1ea9"
      },
      "source": [
        "Let‚Äôs test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Importar el helper para crear el agente\n",
        "from langchain.agents import create_agent\n",
        "\n",
        "# 2. Registrar las tools que tendr√° el agente\n",
        "tools = [retrieve_context]\n",
        "\n",
        "# 3. Prompt del sistema con instrucciones para el modelo\n",
        "prompt = (\n",
        "    \"You have access to a tool that retrieves context from a blog post. \"\n",
        "    \"Use the tool to help answer user queries.\"\n",
        ")\n",
        "\n",
        "# 4. Crear el agente (asumiendo que ya tienes un `model` definido antes)\n",
        "agent = create_agent(model, tools, system_prompt=prompt)\n"
      ],
      "metadata": {
        "id": "qPi7hJW6iWV6"
      },
      "id": "qPi7hJW6iWV6",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "beead75b",
      "metadata": {
        "id": "beead75b"
      },
      "outputs": [],
      "source": [
        "query = (\n",
        "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
        "    \"Once you get the answer, look up common extensions of that method.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
        "response[\"messages\"][-1].pretty_print()\n"
      ],
      "metadata": {
        "id": "3vy9gEdZoQWA"
      },
      "id": "3vy9gEdZoQWA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "83b5bf53",
      "metadata": {
        "id": "83b5bf53"
      },
      "source": [
        "Note that the agent:\n",
        "1. Generates a query to search for a standard method for task decomposition.\n",
        "2. Receiving the answer, generates a second query to search for common extensions of it.\n",
        "3. Having received all necessary context, answers the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f507090",
      "metadata": {
        "id": "9f507090"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}