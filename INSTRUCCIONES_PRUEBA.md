# ğŸ§ª Instrucciones para Probar las Mejoras

## âš¡ Prueba RÃ¡pida (Sin Dependencias Completas)

Para validar que el chunking semÃ¡ntico y filtrado funcionen correctamente:

```bash
python3 test_improvements.py
```

**Esto probarÃ¡:**
- âœ… DetecciÃ³n de secciones (referencias, agradecimientos, apÃ©ndices)
- âœ… Filtrado de contenido de bajo valor
- âœ… Chunking semÃ¡ntico respetando pÃ¡rrafos
- âœ… ProtecciÃ³n de abreviaciones cientÃ­ficas (e.g., i.e., pH, aW, etc.)
- âœ… Overlap inteligente entre chunks

**Resultado esperado:**
```
ğŸ§ª PRUEBAS DE MEJORAS DEL SISTEMA

============================================================
TEST 1: DETECCIÃ“N DE SECCIONES
============================================================
ğŸ“Š EstadÃ­sticas de Filtrado:
  - Caracteres originales: 1029
  - Caracteres filtrados: 618
  - ReducciÃ³n: ~40%
  - Secciones removidas: 3
...
âœ… TODAS LAS PRUEBAS COMPLETADAS
```

---

## ğŸ”¬ Prueba Completa con PDFs

### Paso 1: Instalar Dependencias

```bash
pip install -r requirements.txt
```

O si prefieres usar un entorno virtual:

```bash
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Paso 2: Ejecutar Pipeline Completo

```bash
python run_pipeline.py
```

**Esto ejecutarÃ¡:**
1. Ingesta de PDFs desde `data/pdfs/`
2. Limpieza y **filtrado de secciones** (referencias, tablas, etc.)
3. **Chunking semÃ¡ntico** (respeta pÃ¡rrafos y oraciones)
4. ExtracciÃ³n de metadata
5. VectorizaciÃ³n e indexaciÃ³n
6. BÃºsquedas de prueba

**Salida esperada:**
```
ğŸ§¬ PRESERV-RAG - PIPELINE BASELINE (HITO 1)
============================================================

1ï¸âƒ£  INGESTA DE PDFs
------------------------------------------------------------
âœ“ 18 PDFs cargados
  - Total pÃ¡ginas: XXX
  - Total caracteres: XXX,XXX

2ï¸âƒ£  EXTRACCIÃ“N, LIMPIEZA Y FILTRADO DE TEXTO
------------------------------------------------------------
âœ“ Texto procesado (limpieza + filtrado)
  - ReducciÃ³n de caracteres: XX%
  - Caracteres originales: XXX,XXX
  - Caracteres finales: XXX,XXX

  ğŸ“ Secciones filtradas:
    - Referencias: XX
    - Tablas: XX
    - Agradecimientos: XX
    - ApÃ©ndices: XX
    - Headers/Footers: XXX

3ï¸âƒ£  DIVISIÃ“N EN CHUNKS (SEMÃNTICO)
------------------------------------------------------------
âœ“ Documentos divididos en chunks semÃ¡nticos
  - Total chunks: XXX
  - Documentos Ãºnicos: 18
  - TamaÃ±o promedio: ~500 caracteres
  - TamaÃ±o mÃ­n/mÃ¡x: XX/XXX
  - Chunks por documento: XX
  - PÃ¡rrafos por chunk: ~X.X

...

âœ… PIPELINE COMPLETADO EXITOSAMENTE

ğŸ†• MEJORAS IMPLEMENTADAS:
  âœ“ Filtrado de secciones de bajo valor
  âœ“ Chunking semÃ¡ntico
  âœ“ Overlap inteligente
  âœ“ No corta en abreviaciones cientÃ­ficas
```

### Paso 3: Interfaz GrÃ¡fica con Streamlit

```bash
streamlit run streamlit_app.py
```

**En el navegador (http://localhost:8501):**

1. **Sidebar - ConfiguraciÃ³n:**
   - âœ… Marcar "Filtrar secciones de bajo valor"
   - âœ… Seleccionar quÃ© filtrar (referencias, tablas, etc.)
   - Ajustar tamaÃ±o de chunk y overlap

2. **Tab "Pipeline":**
   - Ejecutar paso a paso
   - Ver estadÃ­sticas de filtrado en tiempo real
   - Observar chunks generados

3. **Tab "BÃºsqueda":**
   - Probar queries
   - Verificar que no se recuperen chunks de referencias

4. **Tab "MÃ©tricas Baseline":**
   - Evaluar con queries de prueba
   - Comparar mÃ©tricas

---

## ğŸ” ValidaciÃ³n de Mejoras

### Verificar Filtrado Funciona

DespuÃ©s de ejecutar `run_pipeline.py`, busca en la salida:

```
ğŸ“ Secciones filtradas:
  - Referencias: > 0
  - Tablas: > 0
  - Agradecimientos: > 0
```

Si los nÃºmeros son > 0, el filtrado estÃ¡ funcionando.

### Verificar Chunking SemÃ¡ntico

Busca en la salida:

```
- PÃ¡rrafos por chunk: ~2.5 (o similar)
```

Esto indica que los chunks respetan pÃ¡rrafos completos.

### Verificar No Se Cortan Abreviaciones

Ejecuta:
```bash
python3 test_improvements.py
```

Y revisa el **TEST 3** - no debe haber advertencias.

---

## ğŸ“Š Comparar con Baseline Original

### OpciÃ³n 1: Deshabilitar Mejoras en Streamlit

1. Abrir Streamlit
2. **Desmarcar** "Filtrar secciones de bajo valor" en sidebar
3. Ejecutar pipeline
4. Guardar mÃ©tricas

Luego:
5. **Marcar** "Filtrar secciones de bajo valor"
6. Ejecutar pipeline nuevamente
7. Comparar mÃ©tricas

### OpciÃ³n 2: Usar CÃ³digo

```python
# Baseline (sin filtrado)
extractor_baseline = TextExtractor(filter_sections=False)
chunker_baseline = DocumentChunker(chunk_size=500, overlap=50)

# Mejorado (con filtrado)
extractor_improved = TextExtractor(filter_sections=True)
chunker_improved = DocumentChunker(chunk_size=500, overlap=50, min_chunk_size=100)

# Comparar estadÃ­sticas
```

---

## ğŸ› Troubleshooting

### Error: "ModuleNotFoundError: No module named 'PyPDF2'"

**SoluciÃ³n:**
```bash
pip install -r requirements.txt
```

### Error: "FileNotFoundError: Carpeta no encontrada: data/pdfs"

**SoluciÃ³n:**
Verifica que exista la carpeta y tenga PDFs:
```bash
ls data/pdfs/*.pdf
```

### Los tests pasan pero el pipeline falla

**Causa probable:** Dependencias no instaladas completamente.

**SoluciÃ³n:**
```bash
pip install PyPDF2 sentence-transformers chromadb streamlit pandas numpy python-dotenv
```

### Chunks muy pequeÃ±os o muy grandes

**SoluciÃ³n:** Ajustar parÃ¡metros en `run_pipeline.py`:
```python
chunk_size = 500     # Cambiar a 300, 700, etc.
overlap = 50         # Cambiar a 30, 100, etc.
min_chunk_size = 100 # Cambiar a 50, 150, etc.
```

---

## ğŸ“ Notas Importantes

1. **Primera ejecuciÃ³n:** El pipeline descargarÃ¡ el modelo de embeddings (~22 MB). Esto puede tomar 1-2 minutos.

2. **VectorizaciÃ³n:** Con 18 PDFs, puede tomar 3-5 minutos crear la base vectorial completa.

3. **ConfiguraciÃ³n por defecto:** El sistema filtra TODAS las secciones de bajo valor por defecto. Esto es intencional para maximizar la calidad.

4. **Memoria:** El proceso puede usar ~500 MB - 1 GB de RAM al vectorizar todos los PDFs.

5. **Base de datos:** Se crea en `data/chroma_db/`. Puedes borrarla y recrearla cuando quieras:
   ```bash
   rm -rf data/chroma_db
   python run_pipeline.py
   ```

---

## âœ… Checklist de ValidaciÃ³n

DespuÃ©s de probar, verifica:

- [ ] `test_improvements.py` pasa exitosamente
- [ ] `run_pipeline.py` completa sin errores
- [ ] Se muestran estadÃ­sticas de secciones filtradas
- [ ] Los chunks tienen tamaÃ±os razonables (100-600 caracteres)
- [ ] El chunking respeta pÃ¡rrafos (avg_paragraphs_per_chunk > 1)
- [ ] Streamlit se ejecuta correctamente
- [ ] Las bÃºsquedas NO retornan chunks de referencias
- [ ] Los chunks contienen informaciÃ³n cientÃ­fica Ãºtil (pH, aW, microorganismos)

---

## ğŸ“ Soporte

Si encuentras problemas, revisa:

1. **MEJORAS_CHUNKING.md** - DocumentaciÃ³n completa
2. **PROBLEMAS_COMUNES.md** - Soluciones a errores comunes
3. **Logs del pipeline** - Ejecutar con output completo

---

**Â¡Listo para probar!** ğŸš€

Empieza con `python3 test_improvements.py` para una validaciÃ³n rÃ¡pida.
