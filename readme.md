# RAG â€“ Hito 2 | PresentaciÃ³n Final

Este repositorio implementa una soluciÃ³n completa de **Retrieval-Augmented Generation (RAG)** que se ejecuta **localmente**, con una **interfaz de usuario en Streamlit** que permite visualizar y probar todo el pipeline de forma interactiva.

El objetivo del proyecto es demostrar, de manera modular y reproducible, las tres etapas fundamentales de un sistema RAG:
1. **Ingesta de documentos**
2. **RecuperaciÃ³n de contexto**
3. **GeneraciÃ³n de respuestas con un LLM local**

---

## ğŸ§© Arquitectura general

La soluciÃ³n estÃ¡ diseÃ±ada siguiendo un enfoque **modular**, donde cada componente del pipeline estÃ¡ desacoplado y puede evaluarse de forma independiente.

### Pipeline RAG
1. **Ingesta**
   - Carga de documentos PDF
   - ExtracciÃ³n de texto
   - SegmentaciÃ³n en fragmentos (*chunking*)
   - Persistencia del Ã­ndice

2. **RecuperaciÃ³n**
   - RecuperaciÃ³n basada en similitud vectorial
   - RecuperaciÃ³n lÃ©xica con BM25
   - Estrategia hÃ­brida (vector + BM25)

3. **GeneraciÃ³n**
   - Uso de un modelo de lenguaje local en formato GGUF
   - GeneraciÃ³n de respuestas restringidas exclusivamente al contexto recuperado

La interfaz Streamlit muestra visualmente estas tres etapas, permitiendo configurar parÃ¡metros y observar los resultados de cada fase.

---

## ğŸ“ Estructura del repositorio

```text
â”œâ”€â”€ app.py                 # Interfaz Streamlit
â”œâ”€â”€ requirements.txt       # Dependencias del proyecto
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/              # Documentos PDF de entrada
â”‚   â””â”€â”€ indexes/           # Ãndices persistidos del sistema
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py          # ConfiguraciÃ³n centralizada
â”‚   â”œâ”€â”€ ingestion.py       # Ingesta y construcciÃ³n del Ã­ndice
â”‚   â”œâ”€â”€ retrieval.py       # RecuperaciÃ³n de contexto
â”‚   â”œâ”€â”€ generation.py      # GeneraciÃ³n con LLM local
â”‚   â””â”€â”€ utils.py           # Funciones auxiliares
â”‚
â””â”€â”€ notebooks/             # Notebooks de experimentaciÃ³n (opcional)
````

---

## âš™ï¸ Requisitos

* Python 3.9 o superior
* EjecuciÃ³n local (no depende de APIs externas)
* Modelo de lenguaje local en formato **GGUF**

---

## ğŸ“¦ InstalaciÃ³n

Clonar el repositorio y crear un entorno virtual:

```bash
python -m venv .venv
```

Activar el entorno:

* **Windows**

```bash
.venv\Scripts\activate
```

* **Linux / Mac**

```bash
source .venv/bin/activate
```

Instalar dependencias:

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ EjecuciÃ³n de la aplicaciÃ³n

1. Colocar los documentos PDF en la carpeta:

```text
data/pdfs/
```

2. Ejecutar la aplicaciÃ³n Streamlit:

```bash
streamlit run app.py
```

3. En la interfaz:

   * Configurar rutas y parÃ¡metros desde la barra lateral
   * (Opcional) reprocesar los documentos
   * Ingresar una pregunta y observar:

     * Contexto recuperado
     * Respuesta generada
     * Fuentes utilizadas

---

## ğŸ§  Modelo de lenguaje

El sistema utiliza un **LLM local en formato GGUF**, cargado dinÃ¡micamente desde la ruta configurada en la interfaz.

> **Nota:**
> El archivo del modelo no se incluye en el repositorio debido a su tamaÃ±o.
> La ruta al modelo se especifica directamente en la interfaz Streamlit.

---

## ğŸ” Reproducibilidad

* El proyecto es completamente reproducible en entorno local
* No requiere conexiÃ³n a servicios externos
* Los Ã­ndices generados se almacenan en `data/indexes/`
* La modularizaciÃ³n permite modificar o extender cada etapa del pipeline

---

## ğŸ“Œ Observaciones finales

Este proyecto demuestra el funcionamiento end-to-end de un sistema RAG, haciendo Ã©nfasis en:

* SeparaciÃ³n clara de responsabilidades
* Transparencia del proceso de recuperaciÃ³n
* ReducciÃ³n de alucinaciones mediante generaciÃ³n condicionada al contexto

```

---
