{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNZMIA7k3iKWYqIsY7Mg1j5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nalpata/proyecto_aplicado_preservantes/blob/main/notebooks/proyecto_1_hito_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HITO 1. RAG Baseline- Proyecto Aplicado: Preservantes\n",
        "\n",
        "En este notebook construimos el baseline de un sistema RAG usando un conjunto de PDFs\n",
        "sobre preservantes. Incluye:\n",
        "\n",
        "1. Carga e ingesta de PDFs\n",
        "2. Preprocesamiento b√°sico y chunking\n",
        "3. Generaci√≥n de embeddings\n",
        "4. Creaci√≥n de un vector store\n",
        "5. Retriever (similarity search)\n",
        "6. Benchmark (Precision@k sobre un set de preguntas)\n"
      ],
      "metadata": {
        "id": "5aDwAGIuz_1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Instalaci√≥n de librer√≠as (celda de c√≥digo)\n",
        "!pip install -q langchain langchain-community langchain-text-splitters \\\n",
        "               chromadb sentence-transformers pypdf\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9Qjmzf770Mw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Importaciones y configuraci√≥n b√°sica\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Para evaluaci√≥n b√°sica\n",
        "from typing import List, Dict\n",
        "import numpy as np\n",
        "\n",
        "# Para ver resultados\n",
        "from pprint import pprint\n"
      ],
      "metadata": {
        "id": "qg7uO2fN0jCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RESETEAR TODO PARA PARTIR LIMPIO EN COLAB\n",
        "\n",
        "import os, shutil\n",
        "\n",
        "# 1) Ir a /content\n",
        "%cd /content\n",
        "\n",
        "# 2) Borrar cualquier clone previo duplicado\n",
        "if os.path.exists(\"proyecto_aplicado_preservantes\"):\n",
        "    shutil.rmtree(\"proyecto_aplicado_preservantes\")\n",
        "    print(\"üóëÔ∏è Carpeta borrada: proyecto_aplicado_preservantes\")\n",
        "\n",
        "# 3) Clonar de nuevo desde tu GitHub\n",
        "!git clone https://github.com/nalpata/proyecto_aplicado_preservantes.git\n",
        "\n",
        "# 4) Entrar a la carpeta correcta\n",
        "%cd proyecto_aplicado_preservantes\n",
        "\n",
        "print(\"\\nüéâ Listo. Ahora estamos en el repo correcto sin duplicados.\")\n",
        "!ls\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "g7lcsJzC073x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ruta base del proyecto en Colab\n",
        "BASE_PATH = Path(\"/content/proyecto_aplicado_preservantes\")\n",
        "\n",
        "DATA_PDF_DIR = BASE_PATH / \"data\" / \"pdfs\"          # aqu√≠ PDFs de preservantes\n",
        "CHROMA_DIR   = BASE_PATH / \"chroma_preservantes\"   # carpeta donde se guardar√° el vector store\n",
        "\n",
        "BASE_PATH.mkdir(parents=True, exist_ok=True)\n",
        "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Base path:\", BASE_PATH)\n",
        "print(\"PDF dir:\", DATA_PDF_DIR)\n",
        "print(\"Chroma dir:\", CHROMA_DIR)\n"
      ],
      "metadata": {
        "id": "28Zh6odf0wx_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Carga de documentos (ingesta de PDFs)\n",
        "def load_pdfs(pdf_dir: Path):\n",
        "    \"\"\"\n",
        "    Carga todos los PDFs de una carpeta usando LangChain.\n",
        "    Devuelve una lista de Documents.\n",
        "    \"\"\"\n",
        "    loader = DirectoryLoader(\n",
        "        str(pdf_dir),\n",
        "        glob=\"*.pdf\",\n",
        "        loader_cls=PyPDFLoader,\n",
        "        show_progress=True\n",
        "    )\n",
        "    docs = loader.load()\n",
        "    return docs\n",
        "\n",
        "raw_docs = load_pdfs(DATA_PDF_DIR)\n",
        "len(raw_docs), raw_docs[0]\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TzhEq3kj1Nbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Preprocesamiento\n",
        "def clean_metadata(docs):\n",
        "    \"\"\"\n",
        "    Normaliza  los metadatos: agrega un campo 'source'\n",
        "    y mantiene solo lo relevante.\n",
        "    \"\"\"\n",
        "    cleaned = []\n",
        "    for d in docs:\n",
        "        meta = d.metadata or {}\n",
        "        source = meta.get(\"source\", \"\")\n",
        "        # Nos quedamos con un metadata simple\n",
        "        new_meta = {\n",
        "            \"source\": source,\n",
        "            \"page\": meta.get(\"page\", None)\n",
        "        }\n",
        "        d.metadata = new_meta\n",
        "        cleaned.append(d)\n",
        "    return cleaned\n",
        "\n",
        "docs = clean_metadata(raw_docs)\n",
        "len(docs), docs[0].metadata\n"
      ],
      "metadata": {
        "id": "i0uqQciH1h7c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunking baseline**\n",
        "\n",
        "Usamos RecursiveCharacterTextSplitter con:\n",
        "- chunk_size = 800\n",
        "- chunk_overlap = 200\n",
        "\n"
      ],
      "metadata": {
        "id": "xQ7macp-2VBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Chunking\n",
        "##Usamos RecursiveCharacterTextSplitter como baseline.\n",
        "\n",
        "CHUNK_SIZE = 800\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "len(chunks), chunks[0]\n"
      ],
      "metadata": {
        "id": "prHZWuWd10br",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##cu√°ntos PDFs y de qu√© archivo vienen los chunks\n",
        "from collections import Counter\n",
        "\n",
        "print(\"N¬∞ de documentos originales:\", len(raw_docs))\n",
        "print(\"Fuentes (PDFs) originales:\")\n",
        "for src in sorted({d.metadata.get(\"source\") for d in raw_docs}):\n",
        "    print(\" -\", src)\n",
        "\n",
        "print(\"\\nN¬∞ de chunks:\", len(chunks))\n",
        "print(\"N¬∞ de chunks por PDF:\")\n",
        "conteo = Counter(d.metadata.get(\"source\") for d in chunks)\n",
        "for src, c in conteo.items():\n",
        "    print(f\"{src}: {c}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GuJ2XgvvCq9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos chunking gerarquico porque los chunks no estan balanceados y se genera un pdf dominante"
      ],
      "metadata": {
        "id": "4V6clGR5Fcib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Uso chunking gerarquico\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from uuid import uuid4\n",
        "\n",
        "# Splitter de nivel alto (bloques grandes)\n",
        "high_level_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "# Splitter de nivel bajo (para el vector store)\n",
        "low_level_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=700,\n",
        "    chunk_overlap=150,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "def hierarchical_chunk(docs):\n",
        "    \"\"\"\n",
        "    1. Divide en bloques grandes (nivel 1)\n",
        "    2. Cada bloque grande se subdivide en chunks peque√±os (nivel 2)\n",
        "    3. A√±ade metadatos de jerarqu√≠a (parent_id, level1_index)\n",
        "    \"\"\"\n",
        "    level1_docs = high_level_splitter.split_documents(docs)\n",
        "\n",
        "    final_chunks = []\n",
        "    for idx, d in enumerate(level1_docs):\n",
        "        parent_id = str(uuid4())  # id √∫nico del bloque grande\n",
        "\n",
        "        # subdividir este bloque\n",
        "        sub_docs = low_level_splitter.split_documents([d])\n",
        "\n",
        "        for s in sub_docs:\n",
        "            meta = dict(s.metadata)\n",
        "            meta[\"parent_id\"] = parent_id\n",
        "            meta[\"level1_index\"] = idx\n",
        "            s.metadata = meta\n",
        "            final_chunks.append(s)\n",
        "\n",
        "    return final_chunks\n",
        "\n",
        "hier_chunks = hierarchical_chunk(docs)\n",
        "len(hier_chunks), hier_chunks[0].metadata\n"
      ],
      "metadata": {
        "id": "NdT2ix_4EbhA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelo de embeddings**"
      ],
      "metadata": {
        "id": "brXykKpx2rlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/distiluse-base-multilingual-cased-v2\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME\n",
        ")\n",
        "\n",
        "print(\"Modelo cargado:\", EMBEDDING_MODEL_NAME)\n"
      ],
      "metadata": {
        "id": "IiTsG4nA2jrm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Store**"
      ],
      "metadata": {
        "id": "MxIQZoms29VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=str(CHROMA_DIR)\n",
        ")\n",
        "\n",
        "vector_store.persist()\n",
        "print(\"Vector store creado con\", vector_store._collection.count(), \"documentos\")\n"
      ],
      "metadata": {
        "id": "rAVAjXe022LH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "BASE_PATH = Path(\"/content/proyecto_aplicado_preservantes\")\n",
        "CHROMA_HIER_DIR = BASE_PATH / \"chroma_preservantes_hier\"\n",
        "\n",
        "import shutil\n",
        "shutil.rmtree(CHROMA_HIER_DIR, ignore_errors=True)\n",
        "CHROMA_HIER_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "vector_store_hier = Chroma.from_documents(\n",
        "    documents=hier_chunks,\n",
        "    embedding=embeddings,                  # el mismo modelo multiling√ºe\n",
        "    persist_directory=str(CHROMA_HIER_DIR)\n",
        ")\n",
        "\n",
        "vector_store_hier.persist()\n",
        "print(\"Vector store HIER creado con\", vector_store_hier._collection.count(), \"documentos\")\n"
      ],
      "metadata": {
        "id": "b59Si2a2FPAR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "#Cuando uso el jer√°rquico\n",
        "CHROMA_HIER_DIR.mkdir(parents=True, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "tvh1gplgRCpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_hier = vector_store_hier.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\"k\": 5, \"fetch_k\": 20}\n",
        ")\n"
      ],
      "metadata": {
        "id": "loYBMp6jHDhc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inspeccionar_query_con(retriever, query: str, k: int = 5):\n",
        "    docs = retriever.invoke(query)[:k]\n",
        "    print(\"Query:\", query, \"\\n\")\n",
        "    for i, d in enumerate(docs, 1):\n",
        "        print(f\"--- Documento {i} ---\")\n",
        "        print(\"Source:\", d.metadata.get(\"source\"), \"| Page:\", d.metadata.get(\"page\"),\n",
        "              \"| level1_index:\", d.metadata.get(\"level1_index\"))\n",
        "        print(d.page_content[:500], \"...\\n\")\n",
        "\n",
        "inspeccionar_query_con(retriever_hier, \"¬øWhat are the antimicrobial effects of sodium benzoate, sodium nitrite, and potassium sorbate?\")\n"
      ],
      "metadata": {
        "id": "VjdAGP91HLu8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "vector_store_hier = Chroma.from_documents(\n",
        "    documents=hier_chunks,    # chunks jer√°rquicos\n",
        "    embedding=embeddings      # mismo modelo multiling√ºe\n",
        ")\n",
        "\n",
        "print(\"Vector store jer√°rquico creado en memoria con:\",\n",
        "      vector_store_hier._collection.count(), \"chunks\")\n"
      ],
      "metadata": {
        "id": "vEXoW3lmSH5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retriever jerarquico**"
      ],
      "metadata": {
        "id": "tQP7iP_Z7Hmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever jer√°rquico\n",
        "retriever_hier = vector_store_hier.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\"k\": 5, \"fetch_k\": 20}\n",
        ")\n",
        "\n",
        "query_ejemplo = \"¬øQu√© es un preservante y qu√© funci√≥n cumple en alimentos?\"\n",
        "resultados_hier = retriever_hier.invoke(query_ejemplo)\n",
        "\n",
        "len(resultados_hier), resultados_hier[0]\n"
      ],
      "metadata": {
        "id": "zDf5OUf36fKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, d in enumerate(resultados_hier, 1):\n",
        "    print(f\"\\n### Documento {i} ###\")\n",
        "    print(\"Source:\", d.metadata.get(\"source\"), \"| Page:\", d.metadata.get(\"page\"))\n",
        "    print(d.page_content[:300], \"...\\n\")\n"
      ],
      "metadata": {
        "id": "ExNn466_7TMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_retriever(retriever, eval_queries, k=5, nombre=\"Evaluaci√≥n\"):\n",
        "    print(f\"\\n=== Evaluando retriever: {nombre} ===\\n\")\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for item in eval_queries:\n",
        "        query = item[\"query\"]\n",
        "        keywords = item[\"relevant_keywords\"]\n",
        "\n",
        "        # Recuperar documentos\n",
        "        docs = retriever.invoke(query)[:k]\n",
        "\n",
        "        # Precision@k manual\n",
        "        hits = 0\n",
        "        for doc in docs:\n",
        "            text = doc.page_content.lower()\n",
        "            if any(kw.lower() in text for kw in keywords):\n",
        "                hits += 1\n",
        "\n",
        "        precision = hits / k\n",
        "        scores.append(precision)\n",
        "\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Precision@{k}: {precision:.2f}\\n\")\n",
        "\n",
        "    print(f\"Precision@{k} promedio: {sum(scores)/len(scores):.2f}\")\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "a3Q5z1HYk0Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive RAG**"
      ],
      "metadata": {
        "id": "cTXJlKGa7iiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 5}\n",
        ")\n"
      ],
      "metadata": {
        "id": "XT1bpwM7jmXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Baseline: solo mostrar textos recuperados\n",
        "def show_retrieval(query: str, k: int = 5, retriever=retriever):\n",
        "    # Con LangChain nuevo el retriever se invoca as√≠:\n",
        "    docs = retriever.invoke(query)\n",
        "    docs = docs[:k]\n",
        "\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    for i, d in enumerate(docs, start=1):\n",
        "        print(f\"--- Documento {i} ---\")\n",
        "        print(\"Source:\", d.metadata.get(\"source\"), \"Page:\", d.metadata.get(\"page\"))\n",
        "        print(d.page_content[:500], \"...\")\n",
        "        print()\n",
        "\n",
        "# Prueba\n",
        "show_retrieval(\"Tipos de preservantes utilizados en bebidas\", retriever=retriever_hier)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DzTmLdl67kcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integramos un LLM para responder**"
      ],
      "metadata": {
        "id": "JeisbG5h9AvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-openai langchain-community openai tiktoken\n"
      ],
      "metadata": {
        "id": "_u_CVpbg9egb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-openai langchain-community langchain-text-splitters\n",
        "!pip install -q langchain-core\n",
        "!pip install -q langchain-experimental\n",
        "!pip install -q langchainhub\n",
        "!pip install -q lc-retrieval\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fOPmeegY-HRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n"
      ],
      "metadata": {
        "id": "GMeWfL_h_Ibi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Ingresa tu OPENAI_API_KEY: \")\n"
      ],
      "metadata": {
        "id": "gCAc7EqO_Lxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n"
      ],
      "metadata": {
        "id": "nvBeg2PA_bkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_rag(query: str, k: int = 5, retriever=retriever_hier):\n",
        "    # 1. Recuperar documentos relevantes\n",
        "    docs = retriever.invoke(query)\n",
        "    docs = docs[:k]\n",
        "\n",
        "    # 2. Construir el contexto a partir de los chunks\n",
        "    context = \"\\n\\n---\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "    # 3. Armar el prompt para el LLM\n",
        "    prompt = f\"\"\"\n",
        "Eres un asistente experto en preservantes de alimentos.\n",
        "Responde usando EXCLUSIVAMENTE la informaci√≥n del contexto.\n",
        "\n",
        "Contexto:\n",
        "{context}\n",
        "\n",
        "Pregunta: {query}\n",
        "\n",
        "Respuesta en espa√±ol, clara y concisa:\n",
        "\"\"\"\n",
        "\n",
        "    # 4. Llamar al modelo\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    # 5. Mostrar resultado y fuentes\n",
        "    print(\"Pregunta:\", query)\n",
        "    print(\"\\n Respuesta:\\n\")\n",
        "    print(response.content)\n",
        "\n",
        "    print(\"\\n Fuentes:\")\n",
        "    for d in docs:\n",
        "        print(\"-\", d.metadata.get(\"source\"), \"| page\", d.metadata.get(\"page\"))\n"
      ],
      "metadata": {
        "id": "oLAzc9bgW5wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask_rag(\"Tipos de preservantes utilizados en bebidas\", retriever=retriever_hier)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NrOhn50DADER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benchmark  (Precision@k)**"
      ],
      "metadata": {
        "id": "TMTkYd5HAhcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_hier = vector_store_hier.as_retriever(\n",
        "    search_type=\"mmr\",          # b√∫squeda diversificada\n",
        "    search_kwargs={\n",
        "        \"k\": 5,                 # n√∫mero final de documentos que regresar√°\n",
        "        \"fetch_k\": 20           # n√∫mero de documentos que explora primero\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "ebOtmiSQXorF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_queries = [\n",
        "{\n",
        "  \"query\": \"¬øQu√© es un preservante antimicrobiano?\",\n",
        "  \"relevant_keywords\": [\n",
        "    \"preservante antimicrobiano\",\n",
        "    \"conservante antimicrobiano\",\n",
        "    \"inhibici√≥n microbiana\",\n",
        "    \"inhibe el crecimiento microbiano\",\n",
        "    \"sustancia antimicrobiana\",\n",
        "    \"agente antimicrobiano\",\n",
        "    \"inhibici√≥n de microorganismos\",\n",
        "\n",
        "    \"antimicrobial preservative\",\n",
        "    \"antimicrobial agent\",\n",
        "    \"microbial growth inhibition\",\n",
        "    \"inhibits microbial growth\"\n",
        "  ]\n",
        "},\n",
        "{\n",
        "  \"query\": \"¬øCu√°les son los factores que afectan la efectividad de los preservantes?\",\n",
        "  \"relevant_keywords\": [\n",
        "    \"efectividad de los preservantes\",\n",
        "    \"factores que afectan la efectividad\",\n",
        "    \"actividad de agua\",\n",
        "    \"aw\",\n",
        "    \"concentraci√≥n del conservante\",\n",
        "    \"concentraci√≥n inhibitoria\",\n",
        "    \"pKa del conservante\",\n",
        "    \"interacci√≥n con composici√≥n del alimento\",\n",
        "\n",
        "    \"preservative effectiveness\",\n",
        "    \"factors influencing preservative efficacy\",\n",
        "    \"water activity\",\n",
        "    \"aw value\",\n",
        "    \"preservative concentration\",\n",
        "    \"food composition interaction\",\n",
        "    \"minimum inhibitory concentration\"\n",
        "  ]\n",
        "},\n",
        "{\n",
        "  \"query\": \"¬øQu√© se entiende por vida √∫til de un alimento?\",\n",
        "  \"relevant_keywords\": [\n",
        "    \"vida √∫til del alimento\",\n",
        "    \"vida √∫til\",\n",
        "    \"deterioro microbiano\",\n",
        "    \"estabilidad del alimento\",\n",
        "    \"seguridad alimentaria\",\n",
        "    \"calidad durante el almacenamiento\",\n",
        "\n",
        "    \"shelf life\",\n",
        "    \"food shelf life\",\n",
        "    \"food spoilage\",\n",
        "    \"microbial spoilage\",\n",
        "    \"quality stability\",\n",
        "    \"storage stability\"\n",
        "  ]\n",
        "}\n",
        "]\n"
      ],
      "metadata": {
        "id": "_Vppu_vwY98m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_hier = evaluate_retriever(\n",
        "    retriever_hier,\n",
        "    eval_queries,\n",
        "    k=5,\n",
        "    nombre=\"Jer√°rquico (MMR + chunking estructural)\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "xaH9fAOOX6NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import numpy as np\n",
        "\n",
        "def precision_at_k(query: str, retrieved_docs: List, keywords: List[str], k: int = 5):\n",
        "    \"\"\"\n",
        "    Calcula Precision@k verificando si los documentos recuperados contienen keywords relevantes.\n",
        "    \"\"\"\n",
        "    hits = 0\n",
        "    for doc in retrieved_docs[:k]:\n",
        "        text = doc.page_content.lower()\n",
        "        # Si alguna keyword aparece en el texto => HIT\n",
        "        if any(keyword.lower() in text for keyword in keywords):\n",
        "            hits += 1\n",
        "\n",
        "    return hits / k  # Precision@k\n",
        "\n",
        "\n",
        "def evaluate_retriever_precision(retriever, eval_queries, k: int = 5, nombre: str = \"Modelo\"):\n",
        "    \"\"\"\n",
        "    Aplica Precision@k a un conjunto de queries y muestra resultados.\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== Evaluando retriever: {nombre} ===\\n\")\n",
        "\n",
        "    scores = []\n",
        "    for item in eval_queries:\n",
        "        query = item[\"query\"]\n",
        "        keywords = item[\"relevant_keywords\"]\n",
        "\n",
        "        # Recuperar documentos\n",
        "        retrieved = retriever.invoke(query)\n",
        "\n",
        "        # Calcular Prec@k\n",
        "        score = precision_at_k(query, retrieved, keywords, k)\n",
        "        scores.append(score)\n",
        "\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Precision@{k}: {score:.2f}\\n\")\n",
        "\n",
        "    print(f\"Precision@{k} promedio: {np.mean(scores):.2f}\")\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "g_jzkS3eZKUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores_hier = evaluate_retriever_precision(\n",
        "    retriever_hier,\n",
        "    eval_queries,\n",
        "    k=5,\n",
        "    nombre=\"Jer√°rquico (MMR)\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "zDhMS1hyZO1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUSIONES**\n",
        "\n",
        "Corpus heterog√©neo (ingl√©s/espa√±ol): Los documentos contienen conceptos relevantes en distintos idiomas, lo que afecta la recuperaci√≥n cuando la evaluaci√≥n depende de keywords √∫nicamente en espa√±ol o traducciones exactas.\n",
        "\n",
        "Evaluaci√≥n basada en coincidencia de palabras clave: Precision@k penaliza documentos que son relevantes conceptualmente, pero no contienen literalmente las palabras clave definidas.\n",
        "\n",
        "Preguntas conceptuales dif√≠ciles: Consultas de tipo ‚Äú¬øQu√© es‚Ä¶?‚Äù requieren definiciones expl√≠citas que pueden no aparecer como tal en el corpus o estar formuladas con vocabulario t√©cnico, reduciendo la recuperaci√≥n efectiva.\n",
        "\n",
        "Tama√±o y calidad del corpus: Aunque el corpus es valioso, varias fuentes no est√°n estructuradas pedag√≥gicamente y contienen tablas, f√≥rmulas o p√°rrafos extensos, lo que dificulta la segmentaci√≥n √≥ptima.\n",
        "\n",
        "**POSIBLES MEJORAS PARA SIGUENTE HITO**\n",
        "\n",
        "Mejorar los embeddings. Adoptar un modelo m√°s robusto y cient√≠fico multiling√ºe\n",
        "\n",
        "Optimizar el proceso de chunking: Usar chunking h√≠brido (estructura + sem√°ntica + tama√±o).\n",
        "\n",
        "Incluir metadatos expl√≠citos (subt√≠tulos, figuras, secciones) para mejorar contexto jer√°rquico.\n",
        "\n",
        "Mejorar la evaluaci√≥n: Expandir keywords con sin√≥nimos y variaciones t√©cnicas.\n",
        "\n"
      ],
      "metadata": {
        "id": "ISl_Iba9aqgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HITO 1 ‚Äî resultados congelados\n",
        "\n",
        "\n",
        "BASE_K = 5\n",
        "\n",
        "baseline_scores = evaluate_retriever_precision(\n",
        "    retriever,\n",
        "    eval_queries,\n",
        "    k=BASE_K,\n",
        "    nombre=\"Hito 1 - Baseline Naive\"\n",
        ")\n",
        "\n",
        "baseline_precision = float(np.mean(baseline_scores))\n",
        "\n",
        "print(\"\\n BASELINE CONGELADO\")\n",
        "print(f\"Precision@{BASE_K} = {baseline_precision:.4f}\")\n"
      ],
      "metadata": {
        "id": "u155LGxSS3oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "pd.DataFrame([{\n",
        "    \"modelo\": \"Hito 1 - Baseline Naive\",\n",
        "    \"Precision@5\": baseline_precision\n",
        "}]).to_csv(\"results/hito1_baseline.csv\", index=False)\n",
        "\n",
        "print(\" Baseline guardado en results/hito1_baseline.csv\")\n"
      ],
      "metadata": {
        "id": "mI5dZ_rvSiye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HITO 2** RAG Baseline MEJORADO- Proyecto Aplicado: Preservantes"
      ],
      "metadata": {
        "id": "oHlVXZ50Ur5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1 Instalar y definir paths**"
      ],
      "metadata": {
        "id": "6TRdjioaabj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain langchain-community langchain-text-splitters chromadb sentence-transformers pypdf\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "BASE_PATH = Path(\"/content/proyecto_aplicado_preservantes\")\n",
        "CHROMA_HIER_DIR = BASE_PATH / \"chroma_preservantes_hier\"   # <- el que usaste en Hito 1\n",
        "RESULTS_DIR = BASE_PATH / \"results\"\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"BASE_PATH:\", BASE_PATH)\n",
        "print(\"CHROMA_HIER_DIR exists?:\", CHROMA_HIER_DIR.exists())\n",
        "print(\"RESULTS_DIR:\", RESULTS_DIR)\n"
      ],
      "metadata": {
        "id": "8uAH1gxFaSn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Cargar embeddings**"
      ],
      "metadata": {
        "id": "1QqI-b5canoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/distiluse-base-multilingual-cased-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
        "\n",
        "print(\"Embeddings:\", EMBEDDING_MODEL_NAME)\n"
      ],
      "metadata": {
        "id": "Oa9aM4dBahtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3 Cargar el vector store jer√°rquico**"
      ],
      "metadata": {
        "id": "PZQIPF2Ya0Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "vector_store_hier = Chroma(\n",
        "    persist_directory=str(CHROMA_HIER_DIR),\n",
        "    embedding_function=embeddings,\n",
        ")\n",
        "\n",
        "print(\"Loaded Chroma collection size:\", vector_store_hier._collection.count())\n"
      ],
      "metadata": {
        "id": "fukiKaI-a2bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.Funciones de evaluacion Hito 1**"
      ],
      "metadata": {
        "id": "Zk3Plrc_bPP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "def precision_at_k(retrieved_docs: List, keywords: List[str], k: int = 5) -> float:\n",
        "    hits = 0\n",
        "    for doc in retrieved_docs[:k]:\n",
        "        text = (doc.page_content or \"\").lower()\n",
        "        if any(keyword.lower() in text for keyword in keywords):\n",
        "            hits += 1\n",
        "    return hits / k\n",
        "\n",
        "def evaluate_retriever_precision(retriever, eval_queries, k: int = 5, nombre: str = \"Modelo\"):\n",
        "    print(f\"\\n=== Evaluando retriever: {nombre} ===\\n\")\n",
        "    scores = []\n",
        "    for item in eval_queries:\n",
        "        query = item[\"query\"]\n",
        "        keywords = item[\"relevant_keywords\"]\n",
        "        retrieved = retriever.invoke(query)\n",
        "        score = precision_at_k(retrieved, keywords, k)\n",
        "        scores.append(score)\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Precision@{k}: {score:.2f}\\n\")\n",
        "    print(f\"Precision@{k} promedio: {float(np.mean(scores)):.2f}\")\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "Y83hdhj8bMts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.Definir el set eval_queries**"
      ],
      "metadata": {
        "id": "lnBy6hYLbvxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_queries = [\n",
        "{\n",
        "  \"query\": \"¬øQu√© es un preservante antimicrobiano?\",\n",
        "  \"relevant_keywords\": [\n",
        "    \"preservante antimicrobiano\",\n",
        "    \"conservante antimicrobiano\",\n",
        "    \"inhibici√≥n microbiana\",\n",
        "    \"inhibe el crecimiento microbiano\",\n",
        "    \"sustancia antimicrobiana\",\n",
        "    \"agente antimicrobiano\",\n",
        "    \"inhibici√≥n de microorganismos\",\n",
        "\n",
        "    \"antimicrobial preservative\",\n",
        "    \"antimicrobial agent\",\n",
        "    \"microbial growth inhibition\",\n",
        "    \"inhibits microbial growth\"\n",
        "  ]\n",
        "},\n",
        "{\n",
        "  \"query\": \"¬øCu√°les son los factores que afectan la efectividad de los preservantes?\",\n",
        "  \"relevant_keywords\": [\n",
        "    \"efectividad de los preservantes\",\n",
        "    \"factores que afectan la efectividad\",\n",
        "    \"actividad de agua\",\n",
        "    \"aw\",\n",
        "    \"concentraci√≥n del conservante\",\n",
        "    \"concentraci√≥n inhibitoria\",\n",
        "    \"pKa del conservante\",\n",
        "    \"interacci√≥n con composici√≥n del alimento\",\n",
        "\n",
        "    \"preservative effectiveness\",\n",
        "    \"factors influencing preservative efficacy\",\n",
        "    \"water activity\",\n",
        "    \"aw value\",\n",
        "    \"preservative concentration\",\n",
        "    \"food composition interaction\",\n",
        "    \"minimum inhibitory concentration\"\n",
        "  ]\n",
        "},\n",
        "{\n",
        "  \"query\": \"¬øQu√© se entiende por vida √∫til de un alimento?\",\n",
        "  \"relevant_keywords\": [\n",
        "    \"vida √∫til del alimento\",\n",
        "    \"vida √∫til\",\n",
        "    \"deterioro microbiano\",\n",
        "    \"estabilidad del alimento\",\n",
        "    \"seguridad alimentaria\",\n",
        "    \"calidad durante el almacenamiento\",\n",
        "\n",
        "    \"shelf life\",\n",
        "    \"food shelf life\",\n",
        "    \"food spoilage\",\n",
        "    \"microbial spoilage\",\n",
        "    \"quality stability\",\n",
        "    \"storage stability\"\n",
        "  ]\n",
        "}\n",
        "]\n"
      ],
      "metadata": {
        "id": "8rRNd6e_dGtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6 Crear baseline retriever + evaluar + congelar**"
      ],
      "metadata": {
        "id": "yX0IXV6cdUEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_K = 5\n",
        "\n",
        "retriever_base = vector_store_hier.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": BASE_K}\n",
        ")\n",
        "\n",
        "scores_base = evaluate_retriever_precision(\n",
        "    retriever_base,\n",
        "    eval_queries,\n",
        "    k=BASE_K,\n",
        "    nombre=\"Hito 1/2 - Baseline (similarity sobre hier_chunks)\"\n",
        ")\n",
        "\n",
        "baseline_precision = float(np.mean(scores_base))\n",
        "print(\"\\nBASELINE CONGELADO\")\n",
        "print(\"Precision@5 =\", baseline_precision)\n"
      ],
      "metadata": {
        "id": "P6VrIrzTcLNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_path = RESULTS_DIR / \"hito1_baseline.csv\"\n",
        "\n",
        "pd.DataFrame([{\n",
        "    \"modelo\": \"Baseline similarity (hier store)\",\n",
        "    \"embedding_model\": EMBEDDING_MODEL_NAME,\n",
        "    \"k\": BASE_K,\n",
        "    \"precision_at_k\": baseline_precision,\n",
        "}]).to_csv(out_path, index=False)\n",
        "\n",
        "print(\" Baseline guardado en:\", out_path)\n"
      ],
      "metadata": {
        "id": "UJroyVYsdc9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. MEJORA MMR**"
      ],
      "metadata": {
        "id": "fDlBmw6vdrM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_mmr = vector_store_hier.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\"k\": 5, \"fetch_k\": 30, \"lambda_mult\": 0.5}\n",
        ")\n",
        "\n",
        "scores_mmr = evaluate_retriever_precision(\n",
        "    retriever_mmr,\n",
        "    eval_queries,\n",
        "    k=5,\n",
        "    nombre=\"Hito 2 - MMR\"\n",
        ")\n",
        "\n",
        "mmr_precision = float(np.mean(scores_mmr))\n",
        "print(\"Precision@5 (MMR):\", mmr_precision)\n",
        "print(\"Delta vs baseline:\", mmr_precision - baseline_precision)\n"
      ],
      "metadata": {
        "id": "NoZ7xn_Mdk1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_path = RESULTS_DIR / \"hito2_mmr.csv\"\n",
        "\n",
        "pd.DataFrame([{\n",
        "    \"modelo\": \"MMR\",\n",
        "    \"embedding_model\": EMBEDDING_MODEL_NAME,\n",
        "    \"k\": 5,\n",
        "    \"fetch_k\": 30,\n",
        "    \"lambda_mult\": 0.5,\n",
        "    \"precision_at_k\": mmr_precision,\n",
        "    \"delta_vs_baseline\": mmr_precision - baseline_precision\n",
        "}]).to_csv(out_path, index=False)\n",
        "\n",
        "print(\" MMR guardado en:\", out_path)\n"
      ],
      "metadata": {
        "id": "PoE4vyD1d34x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmr_configs = [\n",
        "    {\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.2},\n",
        "    {\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5},\n",
        "    {\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.8},\n",
        "    {\"k\": 5, \"fetch_k\": 50, \"lambda_mult\": 0.2},\n",
        "    {\"k\": 5, \"fetch_k\": 50, \"lambda_mult\": 0.5},\n",
        "    {\"k\": 5, \"fetch_k\": 50, \"lambda_mult\": 0.8},\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for cfg in mmr_configs:\n",
        "    retriever_mmr = vector_store_hier.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs=cfg\n",
        "    )\n",
        "    scores = evaluate_retriever_precision(\n",
        "        retriever_mmr,\n",
        "        eval_queries,\n",
        "        k=cfg[\"k\"],\n",
        "        nombre=f\"MMR k={cfg['k']} fetch_k={cfg['fetch_k']} lambda={cfg['lambda_mult']}\"\n",
        "    )\n",
        "    p = float(np.mean(scores))\n",
        "    rows.append({**cfg, \"precision_at_k\": p, \"delta_vs_baseline\": p - baseline_precision})\n",
        "\n",
        "df_mmr = pd.DataFrame(rows).sort_values(\"precision_at_k\", ascending=False)\n",
        "df_mmr\n"
      ],
      "metadata": {
        "id": "s8BX4Qaneivb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUSION:**\n",
        "\n",
        "MMR no mejora Precision@5 en este dominio debido a la homogeneidad tem√°tica de los documentos y al uso previo de chunking jer√°rquico"
      ],
      "metadata": {
        "id": "K0YBqEgFgCEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Mejora de Retrieval #1: Query processing (preprocesamiento/expansi√≥n)**"
      ],
      "metadata": {
        "id": "ezAgZuENgimX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Funci√≤n de reprocesamiento\n",
        "def normalize_query(q: str) -> str:\n",
        "    q = q.lower().strip()\n",
        "    q = re.sub(r\"\\s+\", \" \", q)\n",
        "    return q\n",
        "\n",
        "DOMAIN_SYNONYMS = {\n",
        "    \"vida √∫til\": [\"shelf life\", \"duraci√≥n\", \"almacenamiento\", \"estabilidad\"],\n",
        "    \"preservante\": [\"conservante\", \"aditivo\", \"preservative\"],\n",
        "    \"antimicrobiano\": [\"antimicrobial\", \"inhibici√≥n microbiana\", \"microorganismos\"],\n",
        "    \"actividad de agua\": [\"aw\", \"water activity\"],\n",
        "}\n",
        "\n",
        "def expand_query(q: str) -> str:\n",
        "    qn = normalize_query(q)\n",
        "    extra = []\n",
        "    for k, syns in DOMAIN_SYNONYMS.items():\n",
        "        if k in qn:\n",
        "            extra.extend(syns)\n",
        "    if extra:\n",
        "        return q + \" | \" + \" \".join(extra)\n",
        "    return q\n"
      ],
      "metadata": {
        "id": "mCYjWh2kgQDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIX: regex para normalize_query\n",
        "import re\n",
        "\n",
        "def normalize_query(q: str) -> str:\n",
        "    q = (q or \"\").lower().strip()\n",
        "    q = re.sub(r\"\\s+\", \" \", q)\n",
        "    return q\n"
      ],
      "metadata": {
        "id": "8BzOD4SVixTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WRAPPERS DE RETRIEVER\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def _call_retriever(r, query: str):\n",
        "    \"\"\"\n",
        "    Llama al retriever sin asumir si tiene .invoke() o .get_relevant_documents().\n",
        "    \"\"\"\n",
        "    if hasattr(r, \"invoke\") and callable(getattr(r, \"invoke\")):\n",
        "        return r.invoke(query)\n",
        "    if hasattr(r, \"get_relevant_documents\") and callable(getattr(r, \"get_relevant_documents\")):\n",
        "        return r.get_relevant_documents(query)\n",
        "    raise AttributeError(\"El retriever no tiene ni .invoke() ni .get_relevant_documents()\")\n",
        "\n",
        "class QueryExpansionRetriever:\n",
        "    def __init__(self, base_retriever, expand_fn):\n",
        "        self.base_retriever = base_retriever\n",
        "        self.expand_fn = expand_fn\n",
        "\n",
        "    def invoke(self, query: str):\n",
        "        q2 = self.expand_fn(query)\n",
        "        return _call_retriever(self.base_retriever, q2)\n",
        "\n",
        "    def get_relevant_documents(self, query: str):\n",
        "        return self.invoke(query)\n",
        "\n",
        "def evaluate_retriever_precision(retriever, eval_queries, k=5, nombre=\"\"):\n",
        "    \"\"\"\n",
        "    Eval√∫a Precision@k usando keywords relevantes.\n",
        "    eval_queries: lista de dicts {\"query\": str, \"relevant_keywords\": [..]}\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== Evaluando retriever: {nombre} ===\\n\")\n",
        "    scores = []\n",
        "\n",
        "    for item in eval_queries:\n",
        "        query = item[\"query\"]\n",
        "        keywords = [kw.lower() for kw in item[\"relevant_keywords\"]]\n",
        "\n",
        "        retrieved = _call_retriever(retriever, query)\n",
        "\n",
        "        # Normaliza a lista de Document\n",
        "        if retrieved is None:\n",
        "            retrieved = []\n",
        "        if not isinstance(retrieved, list):\n",
        "            retrieved = list(retrieved)\n",
        "\n",
        "        topk = retrieved[:k]\n",
        "\n",
        "        hits = 0\n",
        "        for doc in topk:\n",
        "            text = \"\"\n",
        "            if hasattr(doc, \"page_content\"):\n",
        "                text = (doc.page_content or \"\").lower()\n",
        "            else:\n",
        "                text = str(doc).lower()\n",
        "\n",
        "            if any(kw in text for kw in keywords):\n",
        "                hits += 1\n",
        "\n",
        "        score = hits / max(k, 1)\n",
        "        scores.append(score)\n",
        "\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Precision@{k}: {score:.2f}\\n\")\n",
        "\n",
        "    print(f\"Precision@{k} promedio: {float(np.mean(scores)):.2f}\")\n",
        "    return scores\n",
        "\n",
        "\n",
        "# Base\n",
        "base_ret = vector_store_hier.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": BASE_K}\n",
        ")\n",
        "\n",
        "# Query expansion\n",
        "retriever_qproc = QueryExpansionRetriever(base_ret, expand_query)\n",
        "\n",
        "scores_qproc = evaluate_retriever_precision(\n",
        "    retriever_qproc,\n",
        "    eval_queries,\n",
        "    k=BASE_K,\n",
        "    nombre=\"Hito 2 - Query Processing (expansion)\"\n",
        ")\n",
        "\n",
        "qproc_precision = float(np.mean(scores_qproc))\n",
        "print(\"\\nP@5 (QueryProc) =\", qproc_precision)\n",
        "print(\"Delta vs baseline =\", qproc_precision - baseline_precision)\n"
      ],
      "metadata": {
        "id": "JV2DYUqcia5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pandas as pd\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "row = {\n",
        "    \"modelo\": \"Hito 2 - Query Processing (expansion)\",\n",
        "    \"k\": 5,\n",
        "    \"precision_at_5\": float(qproc_precision),\n",
        "    \"baseline_precision_at_5\": float(baseline_precision),\n",
        "    \"delta_vs_baseline\": float(qproc_precision - baseline_precision),\n",
        "}\n",
        "\n",
        "df = pd.DataFrame([row])\n",
        "df.to_csv(\"results/hito2_query_processing.csv\", index=False)\n",
        "\n",
        "print(df)\n",
        "print(\" Guardado en results/hito2_query_processing.csv\")\n"
      ],
      "metadata": {
        "id": "NFB5XH4IjiPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUSION:**\n",
        "\n",
        "En promedio, con Query Processing est√°s logrando que 1 de cada 3 resultados en el top-5 sea relevante (vs 1 de cada 4 en el baseline). Para un set peque√±o de queries (3) esto es una se√±al clara de mejora."
      ],
      "metadata": {
        "id": "Vh8WpGJYj7n-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Reranking gratuito usando SentenceTransformers CrossEncoder**"
      ],
      "metadata": {
        "id": "jgrGcEJLl5Jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install sentence-transformers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# 1) Retriever base: traemos m√°s candidatos (fetch_k)\n",
        "FETCH_K = 30   # candidatos iniciales\n",
        "TOP_K   = 5    # lo que devolvemos tras rerank\n",
        "\n",
        "base_ret_fetch = vector_store_hier.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": FETCH_K}\n",
        ")\n",
        "\n",
        "# 2) Modelo de reranking\n",
        "RERANK_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "reranker = CrossEncoder(RERANK_MODEL_NAME, device=device)\n",
        "\n",
        "# 3) Wrapper retriever con .invoke()\n",
        "class RerankRetriever:\n",
        "    def __init__(self, base_retriever, cross_encoder, top_k=5, fetch_k=30):\n",
        "        self.base = base_retriever\n",
        "        self.ce = cross_encoder\n",
        "        self.top_k = top_k\n",
        "        self.fetch_k = fetch_k\n",
        "\n",
        "    def _rerank(self, query, docs):\n",
        "        if len(docs) == 0:\n",
        "            return []\n",
        "        pairs = [(query, d.page_content) for d in docs]\n",
        "        scores = self.ce.predict(pairs)  # array de scores\n",
        "        idx = np.argsort(scores)[::-1][: self.top_k]\n",
        "        return [docs[i] for i in idx]\n",
        "\n",
        "    def invoke(self, query: str):\n",
        "        # base retriever ya trae fetch_k (por search_kwargs)\n",
        "        docs = self.base.invoke(query)\n",
        "        return self._rerank(query, docs)\n",
        "\n",
        "    # compat por si luego usas otros evaluadores LangChain\n",
        "    def get_relevant_documents(self, query: str):\n",
        "        return self.invoke(query)\n",
        "\n",
        "retriever_rerank = RerankRetriever(\n",
        "    base_retriever=base_ret_fetch,\n",
        "    cross_encoder=reranker,\n",
        "    top_k=TOP_K,\n",
        "    fetch_k=FETCH_K\n",
        ")\n",
        "\n",
        "# 4) Evaluaci√≥n\n",
        "scores_rerank = evaluate_retriever_precision(\n",
        "    retriever_rerank,\n",
        "    eval_queries,\n",
        "    k=TOP_K,\n",
        "    nombre=f\"Hito 2 - Reranking (CrossEncoder) fetch_k={FETCH_K}\"\n",
        ")\n",
        "\n",
        "rerank_precision = float(np.mean(scores_rerank))\n",
        "delta = rerank_precision - float(baseline_precision)\n",
        "\n",
        "print(\"\\n RESULTADO RERANKING\")\n",
        "print(\"Precision@5 (Rerank):\", rerank_precision)\n",
        "print(\"Delta vs baseline:\", delta)\n",
        "\n",
        "# 5) Guardar resultados (CSV)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "df = pd.DataFrame([{\n",
        "    \"modelo\": \"Hito 2 - Reranking (CrossEncoder ms-marco-MiniLM-L-6-v2)\",\n",
        "    \"k\": TOP_K,\n",
        "    \"fetch_k\": FETCH_K,\n",
        "    \"precision_at_k\": rerank_precision,\n",
        "    \"baseline_precision_at_k\": float(baseline_precision),\n",
        "    \"delta_vs_baseline\": delta,\n",
        "    \"device\": device\n",
        "}])\n",
        "df.to_csv(\"results/hito2_reranking.csv\", index=False)\n",
        "print(\" Guardado en results/hito2_reranking.csv\")\n",
        "df\n"
      ],
      "metadata": {
        "id": "cr2YDEpTloxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUSION:**\n",
        "\n",
        "La incorporaci√≥n de una etapa de reranking basada en un CrossEncoder permiti√≥ mejorar de manera consistente el desempe√±o del sistema de recuperaci√≥n de informaci√≥n en comparaci√≥n con el baseline definido en el Hito 1. En particular, la m√©trica Precision@5 aument√≥ desde un valor aproximado de 0.27 en el baseline hasta 0.33 tras aplicar reranking, lo que representa una mejora absoluta de +0.0667.\n",
        "\n",
        "Este resultado evidencia que, si bien el vector store jer√°rquico es capaz de recuperar fragmentos relevantes, el orden inicial de los documentos no siempre prioriza aquellos m√°s alineados sem√°nticamente con la intenci√≥n de la consulta. El reranking act√∫asobre esta limitaci√≥n, reevaluando los documentos candidatos mediante un modelo m√°s expresivo que considera de forma conjunta la consulta y cada fragmento recuperado, logrando as√≠ un ordenamiento m√°s preciso en el top-k final."
      ],
      "metadata": {
        "id": "eozoAcepnGc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.HYBRID SEARCH = BM25 + Hybrid (BM25 + Dense jer√°rquico)**"
      ],
      "metadata": {
        "id": "YL3HMsqqnRPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install rank_bm25\n"
      ],
      "metadata": {
        "id": "dK_law1asVPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "# 1) Reconstruir all_splits desde el Chroma ya cargado (vector_store_hier)\n",
        "store_data = vector_store_hier._collection.get(include=[\"documents\", \"metadatas\"])\n",
        "\n",
        "docs = store_data.get(\"documents\", [])\n",
        "metas = store_data.get(\"metadatas\", [])\n",
        "\n",
        "all_splits = [\n",
        "    Document(page_content=txt, metadata=(meta or {}))\n",
        "    for txt, meta in zip(docs, metas)\n",
        "    if txt is not None and str(txt).strip() != \"\"\n",
        "]\n",
        "\n",
        "print(\" all_splits reconstruido desde Chroma. Total:\", len(all_splits))\n",
        "print(\"Ejemplo metadata:\", all_splits[0].metadata)\n"
      ],
      "metadata": {
        "id": "WIF9-HbdkwDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "# seguridad: verificar que all_splits exista\n",
        "assert \"all_splits\" in globals(), \" all_splits no existe. Ejecuta el chunking del Hito 1 primero\"\n",
        "\n",
        "# convertir a Document si fuera string (BM25 lo necesita)\n",
        "if len(all_splits) > 0 and isinstance(all_splits[0], str):\n",
        "    all_splits = [Document(page_content=t) for t in all_splits]\n",
        "\n",
        "print(\" all_splits listo. Tipo:\", type(all_splits[0]))\n"
      ],
      "metadata": {
        "id": "b-gKfCnRrB1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "# 1) Sparse retriever (BM25)\n",
        "bm25_ret = BM25Retriever.from_documents(all_splits)\n",
        "bm25_ret.k = BASE_K  # usa el mismo k del baseline (ej: 5)\n",
        "\n",
        "# 2) Dense retriever (el tuyo: jer√°rquico)\n",
        "dense_ret = vector_store_hier.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": BASE_K}\n",
        ")\n",
        "\n",
        "# 3) Wrapper Hybrid: combina resultados y quita duplicados por (source,page) o por texto\n",
        "def hybrid_invoke(query: str, k: int = BASE_K):\n",
        "    # traemos m√°s candidatos para mezclar\n",
        "    d_docs = dense_ret.get_relevant_documents(query)\n",
        "    b_docs = bm25_ret.get_relevant_documents(query)\n",
        "\n",
        "    merged = []\n",
        "    seen = set()\n",
        "\n",
        "    for doc in (d_docs + b_docs):\n",
        "        src = doc.metadata.get(\"source\", \"\")\n",
        "        page = doc.metadata.get(\"page\", \"\")\n",
        "        key = (src, page, doc.page_content[:200])  # robusto si falta page\n",
        "        if key not in seen:\n",
        "            merged.append(doc)\n",
        "            seen.add(key)\n",
        "\n",
        "    return merged[:k]\n",
        "\n",
        "# 4) Adaptador con .invoke() para tu evaluate_retriever_precision\n",
        "class HybridRetriever:\n",
        "    def __init__(self, k=BASE_K):\n",
        "        self.k = k\n",
        "    def invoke(self, query: str):\n",
        "        return hybrid_invoke(query, k=self.k)\n",
        "\n",
        "hybrid_ret = HybridRetriever(k=BASE_K)\n",
        "\n",
        "# 5) Evaluaci√≥n BM25 solo\n",
        "scores_bm25 = evaluate_retriever_precision(\n",
        "    bm25_ret, eval_queries, k=BASE_K, nombre=f\"Hito 2 - BM25 (k={BASE_K})\"\n",
        ")\n",
        "bm25_precision = float(np.mean(scores_bm25))\n",
        "print(\"P@5 (BM25) =\", bm25_precision, \"Delta =\", bm25_precision - baseline_precision)\n",
        "\n",
        "# 6) Evaluaci√≥n Hybrid\n",
        "scores_hybrid = evaluate_retriever_precision(\n",
        "    hybrid_ret, eval_queries, k=BASE_K, nombre=f\"Hito 2 - HYBRID (BM25 + Dense, k={BASE_K})\"\n",
        ")\n",
        "hybrid_precision = float(np.mean(scores_hybrid))\n",
        "print(\"P@5 (HYBRID) =\", hybrid_precision, \"Delta =\", hybrid_precision - baseline_precision)\n"
      ],
      "metadata": {
        "id": "ZOGzzutRrn5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- helper: llamar retrievers de forma compatible (invoke vs get_relevant_documents) ---\n",
        "def _retrieve_any(retriever, query: str):\n",
        "    if hasattr(retriever, \"invoke\"):\n",
        "        return retriever.invoke(query)\n",
        "    if hasattr(retriever, \"get_relevant_documents\"):\n",
        "        return retriever.get_relevant_documents(query)\n",
        "    raise AttributeError(f\"Retriever sin m√©todo compatible: {type(retriever)}\")\n",
        "\n",
        "# --- Hybrid simple por \"uni√≥n + re-ranking por score\" (sin EnsembleRetriever) ---\n",
        "class HybridUnionRetriever:\n",
        "    def __init__(self, dense_ret, bm25_ret, k=5):\n",
        "        self.dense_ret = dense_ret\n",
        "        self.bm25_ret = bm25_ret\n",
        "        self.k = k\n",
        "\n",
        "    def invoke(self, query: str):\n",
        "        dense_docs = _retrieve_any(self.dense_ret, query)\n",
        "        bm25_docs  = _retrieve_any(self.bm25_ret, query)\n",
        "\n",
        "        # uni√≥n por texto+source para evitar duplicados\n",
        "        seen = set()\n",
        "        merged = []\n",
        "        for d in (dense_docs + bm25_docs):\n",
        "            src = (d.metadata or {}).get(\"source\", \"\")\n",
        "            key = (src, d.page_content[:200])\n",
        "            if key not in seen:\n",
        "                seen.add(key)\n",
        "                merged.append(d)\n",
        "\n",
        "        return merged[: self.k]\n",
        "\n",
        "# --- Evaluaci√≥n P@k compatible ---\n",
        "def evaluate_retriever_precision(retriever, eval_queries, k=5, nombre=\"\"):\n",
        "    scores = []\n",
        "    print(f\"\\n=== Evaluando retriever: {nombre} ===\\n\")\n",
        "    for item in eval_queries:\n",
        "        q = item[\"query\"]\n",
        "        keywords = item[\"relevant_keywords\"]\n",
        "\n",
        "        retrieved = _retrieve_any(retriever, q)\n",
        "        retrieved = retrieved[:k]\n",
        "\n",
        "        # precision@k: cuenta si aparece al menos una keyword en cada doc\n",
        "        hits = 0\n",
        "        for doc in retrieved:\n",
        "            text = (doc.page_content or \"\").lower()\n",
        "            if any(kw.lower() in text for kw in keywords):\n",
        "                hits += 1\n",
        "        p_at_k = hits / k\n",
        "        scores.append(p_at_k)\n",
        "\n",
        "        print(f\"Query: {q}\")\n",
        "        print(f\"Precision@{k}: {p_at_k:.2f}\\n\")\n",
        "\n",
        "    print(f\"Precision@{k} promedio: {float(np.mean(scores)):.2f}\")\n",
        "    return scores\n",
        "\n",
        "# --- Construir Hybrid ---\n",
        "# Asume que ya existen:\n",
        "#   dense_ret = vector_store_hier.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": BASE_K})\n",
        "#   bm25_ret  = BM25Retriever.from_documents(all_splits); bm25_ret.k = BASE_K\n",
        "\n",
        "hybrid_ret = HybridUnionRetriever(dense_ret=dense_ret, bm25_ret=bm25_ret, k=BASE_K)\n",
        "\n",
        "scores_hybrid = evaluate_retriever_precision(\n",
        "    hybrid_ret,\n",
        "    eval_queries,\n",
        "    k=BASE_K,\n",
        "    nombre=f\"Hito 2 - HYBRID (BM25 + Dense, k={BASE_K})\"\n",
        ")\n",
        "\n",
        "hybrid_precision = float(np.mean(scores_hybrid))\n",
        "print(\"\\nP@5 (HYBRID) =\", hybrid_precision, \" Delta =\", hybrid_precision - baseline_precision)\n"
      ],
      "metadata": {
        "id": "o3qH03sQtN1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HITO 2 ‚Äî HYBRID + RERANK (con clase RerankRetriever)\n",
        "\n",
        "# 0) Seguridad: cosas que deben existir\n",
        "assert \"bm25_ret\" in globals(), \"No existe bm25_ret. Ejecuta primero la celda de BM25.\"\n",
        "assert \"vector_store_hier\" in globals(), \"No existe vector_store_hier.\"\n",
        "assert \"RerankRetriever\" in globals(), \"No existe la clase RerankRetriever (la del CrossEncoder). Ejecuta esa celda primero.\"\n",
        "assert \"reranker\" in globals(), \"No existe 'reranker' (tu CrossEncoder). Ejecuta la celda donde creas CrossEncoder.\"\n",
        "assert \"BASE_K\" in globals(), \"No existe BASE_K.\"\n",
        "assert \"evaluate_retriever_precision\" in globals(), \"No existe evaluate_retriever_precision.\"\n",
        "assert \"eval_queries\" in globals(), \"No existe eval_queries.\"\n",
        "assert \"baseline_precision\" in globals(), \"No existe baseline_precision.\"\n",
        "\n",
        "# 1) Dense (jer√°rquico)\n",
        "dense_ret = vector_store_hier.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": BASE_K}\n",
        ")\n",
        "\n",
        "# 2) HYBRID simple que usa .invoke()\n",
        "class HybridInvokeRetriever:\n",
        "    def __init__(self, sparse_retriever, dense_retriever, k=5):\n",
        "        self.sparse = sparse_retriever\n",
        "        self.dense = dense_retriever\n",
        "        self.k = k\n",
        "\n",
        "    def invoke(self, query: str):\n",
        "        docs_sparse = self.sparse.invoke(query)\n",
        "        docs_dense  = self.dense.invoke(query)\n",
        "\n",
        "        # merge + dedupe por (source + snippet)\n",
        "        seen = set()\n",
        "        merged = []\n",
        "        for d in (docs_sparse + docs_dense):\n",
        "            key = (str(d.metadata.get(\"source\",\"\")), d.page_content[:200])\n",
        "            if key not in seen:\n",
        "                seen.add(key)\n",
        "                merged.append(d)\n",
        "\n",
        "        return merged[: self.k]\n",
        "\n",
        "    def get_relevant_documents(self, query: str):\n",
        "        return self.invoke(query)\n",
        "\n",
        "hyb_pool = HybridInvokeRetriever(bm25_ret, dense_ret, k=BASE_K)\n",
        "\n",
        "# 3) Aplicar reranking encima del h√≠brido\n",
        "# Para eso, hacemos un pool \"m√°s grande\" y luego el rerank deja top_k=BASE_K.\n",
        "\n",
        "hyb_pool_big = HybridInvokeRetriever(bm25_ret, dense_ret, k=30)\n",
        "\n",
        "retriever_hybrid_rerank = RerankRetriever(\n",
        "    base_retriever=hyb_pool_big,\n",
        "    cross_encoder=reranker,\n",
        "    top_k=BASE_K,\n",
        "    fetch_k=30\n",
        ")\n",
        "\n",
        "scores_hybrid_rerank = evaluate_retriever_precision(\n",
        "    retriever_hybrid_rerank,\n",
        "    eval_queries,\n",
        "    k=BASE_K,\n",
        "    nombre=f\"Hito 2 - HYBRID + RERANK (k={BASE_K})\"\n",
        ")\n",
        "\n",
        "hybrid_rerank_precision = float(np.mean(scores_hybrid_rerank))\n",
        "print(\"\\nP@5 (HYBRID+RERANK) =\", hybrid_rerank_precision)\n",
        "print(\"Delta vs baseline =\", hybrid_rerank_precision - baseline_precision)\n"
      ],
      "metadata": {
        "id": "CwOj50jvwn-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# asegurar carpeta de resultados\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "# guardar resultados finales del Hito 2\n",
        "results_hito2 = pd.DataFrame([\n",
        "    {\n",
        "        \"modelo\": \"Hito 2 - Hybrid + Rerank\",\n",
        "        \"k\": 5,\n",
        "        \"precision_at_5\": 0.4666666666666666,\n",
        "        \"delta_vs_baseline\": 0.20\n",
        "    }\n",
        "])\n",
        "\n",
        "results_hito2.to_csv(\"results/hito2_hybrid_rerank.csv\", index=False)\n",
        "\n",
        "print(\" Resultados guardados en results/hito2_hybrid_rerank.csv\")\n"
      ],
      "metadata": {
        "id": "S7RbPYPJxWmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUSION:**\n",
        "\n",
        "La incorporaci√≥n de un esquema de Hybrid Retrieval combinado con reranking mediante Cross-Encoder produjo una mejora importante en el desempe√±o del sistema de recuperaci√≥n de informaci√≥n respecto al baseline definido en el Hito 1. Mientras el baseline jer√°rquico basado √∫nicamente en similitud sem√°ntica alcanz√≥ una precisi√≥n@5 de 0.27, la arquitectura Hybrid + Rerank logr√≥ una precisi√≥n@5 de 0.47, representando un incremento absoluto de +0.20.\n",
        "\n",
        "Este resultado evidencia que la combinaci√≥n de se√±ales densas (embeddings sem√°nticos), se√±ales l√©xicas (BM25) y un modelo de reranking supervisado permite priorizar documentos m√°s relevantes en las primeras posiciones del ranking, especialmente en consultas conceptuales."
      ],
      "metadata": {
        "id": "5ZA-YpiSxiPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "results_comparison = pd.DataFrame([\n",
        "    {\"Modelo\": \"Hito 1 - Baseline (Hierarchical Similarity)\", \"Precision@5\": 0.27, \"Delta_vs_Baseline\": 0.00},\n",
        "    {\"Modelo\": \"Hito 2 - MMR\", \"Precision@5\": 0.20, \"Delta_vs_Baseline\": -0.07},\n",
        "    {\"Modelo\": \"Hito 2 - Query Processing (Expansion)\", \"Precision@5\": 0.33, \"Delta_vs_Baseline\": 0.07},\n",
        "    {\"Modelo\": \"Hito 2 - Reranking (CrossEncoder)\", \"Precision@5\": 0.33, \"Delta_vs_Baseline\": 0.07},\n",
        "    {\"Modelo\": \"Hito 2 - BM25\", \"Precision@5\": 0.47, \"Delta_vs_Baseline\": 0.20},\n",
        "    {\"Modelo\": \"Hito 2 - Hybrid (BM25 + Dense)\", \"Precision@5\": 0.27, \"Delta_vs_Baseline\": 0.00},\n",
        "    {\"Modelo\": \"Hito 2 - Hybrid + Rerank\", \"Precision@5\": 0.47, \"Delta_vs_Baseline\": 0.20},\n",
        "])\n",
        "\n",
        "results_comparison.to_csv(\"results/comparacion_hito1_hito2.csv\", index=False)\n",
        "\n",
        "results_comparison\n"
      ],
      "metadata": {
        "id": "MrbS32xHyWmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La Tabla anterior presenta la comparaci√≥n de desempe√±o entre el baseline definido en el Hito 1 y las distintas t√©cnicas de mejora evaluadas en el Hito 2, utilizando la m√©trica Precision@5. El baseline jer√°rquico basado en similitud sem√°ntica alcanz√≥ una precisi√≥n@5 de 0.27, sirviendo como punto de referencia para evaluar el impacto de las t√©cnicas avanzadas de recuperaci√≥n.\n",
        "\n",
        "Los resultados muestran que t√©cnicas como Query Processing mediante expansi√≥n de consultas y Reranking con modelos Cross-Encoder producen mejoras moderadas (+0.07), concluyendo que la reformulaci√≥n de consultas y el reordenamiento supervisado ayudan a priorizar documentos relevantes. Al contrario, el uso de MMR no mejora el desempe√±o, lo que permite concluir que la penalizaci√≥n por redundancia no es beneficiosa para documentos t√©cnicos extensos.\n",
        "\n",
        "El mayor incremento se observa al incorporar recuperaci√≥n l√©xica mediante BM25, alcanzando una precisi√≥n@5 de 0.47 (+0.20). Sin embargo, la combinaci√≥n directa de BM25 con embeddings densos (Hybrid) no genera mejoras adicionales. Finalmente, la arquitectura Hybrid + Rerank logra igualar el mejor desempe√±o observado, consolid√°ndose como la soluci√≥n m√°s robusta al integrar m√∫ltiples se√±ales de recuperaci√≥n y un modelo de reordenamiento supervisado."
      ],
      "metadata": {
        "id": "YAg9JI4CyhEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Pipeline de Generaci√≥n Aumentada (RAG-G)**"
      ],
      "metadata": {
        "id": "foyvHqG3y034"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U llama-cpp-python==0.2.90\n",
        "\n",
        "import os, textwrap\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Modelo GGUF\n",
        "MODEL_URL = \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf\"\n",
        "MODEL_PATH = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
        "\n",
        "# Descargar modelo\n",
        "!wget -q -O {MODEL_PATH} {MODEL_URL}\n",
        "print(\"Descargado:\", os.path.getsize(MODEL_PATH), \"bytes\")\n",
        "\n",
        "# Cargar modelo\n",
        "llama = Llama(\n",
        "    model_path=MODEL_PATH,\n",
        "    n_ctx=4096,\n",
        "    n_threads=8,\n",
        "    n_gpu_layers=35  # si no tienes GPU o falla, pon 0\n",
        ")\n",
        "\n",
        "def llama_generate(prompt, max_tokens=300, temperature=0.2, top_p=0.9):\n",
        "    out = llama(\n",
        "        prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        stop=[\"</s>\"]\n",
        "    )\n",
        "    return out[\"choices\"][0][\"text\"]\n"
      ],
      "metadata": {
        "id": "ZEzZx-Sa3JKg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# 1) Compat: retriever puede ser LangChain (.invoke) o BM25 (.get_relevant_documents)\n",
        "def _retrieve_any(retriever, query: str):\n",
        "    if hasattr(retriever, \"invoke\") and callable(getattr(retriever, \"invoke\")):\n",
        "        return retriever.invoke(query)\n",
        "    if hasattr(retriever, \"get_relevant_documents\") and callable(getattr(retriever, \"get_relevant_documents\")):\n",
        "        return retriever.get_relevant_documents(query)\n",
        "    raise AttributeError(f\"Retriever sin m√©todo compatible: {type(retriever)}\")\n",
        "\n",
        "# 2) Contexto aumentado con metadatos\n",
        "def format_docs_for_context(docs: List[Document], max_chars: int = 12000) -> str:\n",
        "    blocks, total = [], 0\n",
        "    for i, d in enumerate(docs, 1):\n",
        "        meta = d.metadata or {}\n",
        "        source = meta.get(\"source\", meta.get(\"file_name\", \"unknown_source\"))\n",
        "        page = meta.get(\"page\", meta.get(\"page_number\", \"NA\"))\n",
        "        chunk_id = meta.get(\"chunk_id\", meta.get(\"id\", f\"chunk_{i}\"))\n",
        "\n",
        "        text = (d.page_content or \"\").strip()\n",
        "        block = f\"[DOC {i}] source={source} | page={page} | chunk_id={chunk_id}\\n{text}\\n\"\n",
        "        if total + len(block) > max_chars:\n",
        "            break\n",
        "        blocks.append(block)\n",
        "        total += len(block)\n",
        "    return \"\\n---\\n\".join(blocks)\n",
        "\n",
        "# 3) Prompt anti-alucinaci√≥n + citas\n",
        "def build_prompt(question: str, context: str) -> str:\n",
        "    return f\"\"\"\n",
        "Eres un asistente experto en preservantes.\n",
        "REGLAS:\n",
        "- Responde SOLO usando el CONTEXTO.\n",
        "- Si no hay evidencia suficiente, responde: \"No encuentro evidencia suficiente en los documentos.\"\n",
        "- No inventes datos.\n",
        "- Cita con [DOC i] en cada afirmaci√≥n importante.\n",
        "\n",
        "PREGUNTA:\n",
        "{question}\n",
        "\n",
        "CONTEXTO:\n",
        "{context}\n",
        "\n",
        "RESPUESTA (en espa√±ol, clara y estructurada):\n",
        "\"\"\"\n",
        "\n",
        "# 4) Pipeline final\n",
        "def ask_rag_llama_cpp(question: str, retriever, k_retrieval: int = 10, k_context: int = 5, max_chars: int = 12000):\n",
        "    docs = _retrieve_any(retriever, question) or []\n",
        "    docs = docs[:k_retrieval]\n",
        "    docs_ctx = docs[:k_context]\n",
        "\n",
        "    context = format_docs_for_context(docs_ctx, max_chars=max_chars)\n",
        "    prompt = build_prompt(question, context)\n",
        "\n",
        "    answer = llama_generate(prompt, max_tokens=350, temperature=0.2, top_p=0.9)\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"docs_used\": [\n",
        "            {\n",
        "                \"source\": (d.metadata or {}).get(\"source\", (d.metadata or {}).get(\"file_name\", \"unknown_source\")),\n",
        "                \"page\": (d.metadata or {}).get(\"page\", (d.metadata or {}).get(\"page_number\", \"NA\")),\n",
        "                \"chunk_id\": (d.metadata or {}).get(\"chunk_id\", (d.metadata or {}).get(\"id\", None)),\n",
        "                \"snippet\": (d.page_content or \"\")[:250]\n",
        "            }\n",
        "            for d in docs_ctx\n",
        "        ]\n",
        "    }\n"
      ],
      "metadata": {
        "id": "AR5a_nBS5DTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG-G con LLaMA GGUF\n",
        "# - auto-detecta el retriever\n",
        "# - genera respuestas con citas\n",
        "# - guarda resultados (JSON/CSV)\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import List, Any, Dict\n",
        "\n",
        "# ----------------------------\n",
        "# 0) Validaciones m√≠nimas\n",
        "# ----------------------------\n",
        "if \"llama_generate\" not in globals():\n",
        "    raise RuntimeError(\"No encuentro llama_generate(). Ejecuta primero la celda del modelo GGUF (llama-cpp).\")\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Detectar autom√°ticamente el mejor retriever\n",
        "# ----------------------------\n",
        "PREFERRED_RETRIEVER_NAMES = [\n",
        "\n",
        "    \"retriever_rerank\", \"retriever_mmr\", \"retriever_qproc\",\n",
        "    \"retriever_hybrid\", \"hybrid_retriever\",\n",
        "    \"bm25_ret\", \"bm25_retriever\",\n",
        "    \"retriever_base\", \"retriever\",\n",
        "    \"ensemble_retriever\",\n",
        "]\n",
        "\n",
        "def _is_retriever(obj: Any) -> bool:\n",
        "    return (\n",
        "        (hasattr(obj, \"invoke\") and callable(getattr(obj, \"invoke\"))) or\n",
        "        (hasattr(obj, \"get_relevant_documents\") and callable(getattr(obj, \"get_relevant_documents\")))\n",
        "    )\n",
        "\n",
        "def _pick_retriever_from_globals() -> Any:\n",
        "    # 1) por nombre preferido\n",
        "    for name in PREFERRED_RETRIEVER_NAMES:\n",
        "        if name in globals() and _is_retriever(globals()[name]):\n",
        "            return globals()[name], name\n",
        "\n",
        "    # 2) fallback: primer objeto en globals() que parezca retriever\n",
        "    candidates = []\n",
        "    for k, v in globals().items():\n",
        "        if k.startswith(\"_\"):\n",
        "            continue\n",
        "        if _is_retriever(v):\n",
        "            candidates.append((k, v))\n",
        "\n",
        "    if not candidates:\n",
        "        raise RuntimeError(\n",
        "            \"No encontr√© ning√∫n retriever en el notebook. \"\n",
        "            \"Aseg√∫rate de haber creado al menos uno (vectorstore.as_retriever(), BM25Retriever, hybrid, etc.).\"\n",
        "        )\n",
        "\n",
        "    # Heur√≠stica: si el nombre contiene 'rerank' o 'mmr' lo preferimos\n",
        "    def score(name: str) -> int:\n",
        "        s = 0\n",
        "        n = name.lower()\n",
        "        if \"rerank\" in n: s += 50\n",
        "        if \"mmr\" in n: s += 40\n",
        "        if \"hybrid\" in n: s += 30\n",
        "        if \"bm25\" in n: s += 20\n",
        "        if \"base\" in n: s += 10\n",
        "        return s\n",
        "\n",
        "    candidates.sort(key=lambda kv: score(kv[0]), reverse=True)\n",
        "    return candidates[0][1], candidates[0][0]\n",
        "\n",
        "retriever, retriever_name = _pick_retriever_from_globals()\n",
        "print(f\" Usando retriever detectado autom√°ticamente: {retriever_name}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Helpers robustos\n",
        "# ----------------------------\n",
        "def _retrieve_any(retriever: Any, query: str):\n",
        "    if hasattr(retriever, \"invoke\") and callable(getattr(retriever, \"invoke\")):\n",
        "        return retriever.invoke(query)\n",
        "    if hasattr(retriever, \"get_relevant_documents\") and callable(getattr(retriever, \"get_relevant_documents\")):\n",
        "        return retriever.get_relevant_documents(query)\n",
        "    raise AttributeError(f\"Retriever sin m√©todo compatible: {type(retriever)}\")\n",
        "\n",
        "def _get_meta(d):\n",
        "    try:\n",
        "        return d.metadata or {}\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "def format_docs_for_context(docs: List[Any], max_chars: int = 12000) -> str:\n",
        "    blocks, total = [], 0\n",
        "    for i, d in enumerate(docs, 1):\n",
        "        meta = _get_meta(d)\n",
        "        source = meta.get(\"source\", meta.get(\"file_name\", \"unknown_source\"))\n",
        "        page = meta.get(\"page\", meta.get(\"page_number\", \"NA\"))\n",
        "        chunk_id = meta.get(\"chunk_id\", meta.get(\"id\", f\"chunk_{i}\"))\n",
        "        text = (getattr(d, \"page_content\", \"\") or \"\").strip()\n",
        "\n",
        "        block = f\"[DOC {i}] source={source} | page={page} | chunk_id={chunk_id}\\n{text}\\n\"\n",
        "        if total + len(block) > max_chars:\n",
        "            break\n",
        "        blocks.append(block)\n",
        "        total += len(block)\n",
        "\n",
        "    return \"\\n---\\n\".join(blocks)\n",
        "\n",
        "def build_prompt(question: str, context: str) -> str:\n",
        "    return f\"\"\"\n",
        "Eres un asistente experto en preservantes y documentaci√≥n t√©cnica.\n",
        "REGLAS ESTRICTAS:\n",
        "- Responde SOLO usando el CONTEXTO.\n",
        "- Si el contexto no contiene evidencia suficiente, responde exactamente:\n",
        "  \"No encuentro evidencia suficiente en los documentos.\"\n",
        "- No inventes datos, cifras, l√≠mites ni nombres.\n",
        "- Incluye citas [DOC i] en cada afirmaci√≥n importante (definiciones, l√≠mites, efectos, recomendaciones).\n",
        "\n",
        "PREGUNTA:\n",
        "{question}\n",
        "\n",
        "CONTEXTO:\n",
        "{context}\n",
        "\n",
        "RESPUESTA (en espa√±ol, clara; usa bullets si aplica y termina con un resumen de 1-2 l√≠neas):\n",
        "\"\"\"\n",
        "\n",
        "def ask_rag_llama_cpp(question: str, retriever: Any,\n",
        "                      k_retrieval: int = 10, k_context: int = 5,\n",
        "                      max_chars: int = 12000,\n",
        "                      max_tokens: int = 350, temperature: float = 0.2, top_p: float = 0.9) -> Dict[str, Any]:\n",
        "    docs = _retrieve_any(retriever, question) or []\n",
        "    docs = docs[:k_retrieval]\n",
        "    docs_ctx = docs[:k_context]\n",
        "\n",
        "    context = format_docs_for_context(docs_ctx, max_chars=max_chars)\n",
        "    prompt = build_prompt(question, context)\n",
        "\n",
        "    answer = llama_generate(prompt, max_tokens=max_tokens, temperature=temperature, top_p=top_p)\n",
        "\n",
        "    used = []\n",
        "    for d in docs_ctx:\n",
        "        meta = _get_meta(d)\n",
        "        used.append({\n",
        "            \"source\": meta.get(\"source\", meta.get(\"file_name\", \"unknown_source\")),\n",
        "            \"page\": meta.get(\"page\", meta.get(\"page_number\", \"NA\")),\n",
        "            \"chunk_id\": meta.get(\"chunk_id\", meta.get(\"id\", None)),\n",
        "            \"snippet\": (getattr(d, \"page_content\", \"\") or \"\")[:250]\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"retriever_used\": retriever_name,\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"docs_used\": used\n",
        "    }\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Ejecutar prueba + guardar resultados\n",
        "# ----------------------------\n",
        "QUESTIONS = [\n",
        "    \"¬øQu√© factores afectan la efectividad de los preservantes seg√∫n los documentos?\",\n",
        "    \"¬øQu√© riesgos o efectos adversos se mencionan sobre el uso de preservantes y en qu√© condiciones aparecen?\",\n",
        "    \"¬øQu√© recomendaciones, l√≠mites o precauciones de uso se describen para preservantes en alimentos?\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "for q in QUESTIONS:\n",
        "    r = ask_rag_llama_cpp(q, retriever)\n",
        "    results.append(r)\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"PREGUNTA:\", r[\"question\"])\n",
        "    print(\"-\"*90)\n",
        "    print(r[\"answer\"])\n",
        "    print(\"\\nFuentes usadas:\")\n",
        "    for i, d in enumerate(r[\"docs_used\"], 1):\n",
        "        print(f\"  - [DOC {i}] {d['source']} | page {d['page']} | chunk {d['chunk_id']}\")\n",
        "\n",
        "# Guardar JSON\n",
        "json_path = \"results/rag_llama_results.json\"\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Guardar CSV (respuesta + fuentes)\n",
        "rows = []\n",
        "for r in results:\n",
        "    sources = \"; \".join([f\"{d['source']}|p{d['page']}|{d['chunk_id']}\" for d in r[\"docs_used\"]])\n",
        "    rows.append({\n",
        "        \"retriever_used\": r[\"retriever_used\"],\n",
        "        \"question\": r[\"question\"],\n",
        "        \"answer\": r[\"answer\"],\n",
        "        \"sources\": sources\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "csv_path = \"results/rag_llama_results.csv\"\n",
        "df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"\\n Listo. Archivos generados:\")\n",
        "print(\" -\", json_path)\n",
        "print(\" -\", csv_path)\n"
      ],
      "metadata": {
        "id": "YY7MgZqZ5wrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONCLUSION**\n",
        "\n",
        "La implementaci√≥n del pipeline de generaci√≥n aumentada (RAG-G) permiti√≥ mejorar la calidad y trazabilidad de las respuestas generadas a partir de los pdfs cargados. Al integrar un esquema de recuperaci√≥n con re-ranking y un modelo de lenguaje LLaMA (GGUF), se logr√≥ que las respuestas estuvieran expl√≠citamente fundamentadas en el contexto recuperado, incorporando citas a las fuentes originales y reduciendo el riesgo de alucinaciones. El uso de un prompt con reglas estrictas ‚Äîque obliga al modelo a responder √∫nicamente con base en la evidencia disponible‚Äî result√≥ clave para mantener la fidelidad al contenido documental.\n",
        "\n",
        "En t√©rminos de contenido, el sistema fue capaz de identificar de manera consistente los factores que afectan la efectividad de los preservantes, tales como condiciones de uso, caracter√≠sticas del alimento y limitaciones t√©cnicas descritas en los documentos. Para estas preguntas, el pipeline produjo respuestas estructuradas, claras y con respaldo expl√≠cito en las fuentes, lo que evidencia una correcta alineaci√≥n entre la etapa de recuperaci√≥n y la etapa de generaci√≥n.\n",
        "\n",
        "Para las preguntas relacionadas con riesgos o efectos adversos, el modelo respondi√≥ indicando la ausencia de evidencia suficiente en los documentos disponibles. Este comportamiento que se busca en los sistemas RAG, ya que demuestra que el pipeline prioriza la veracidad y la evidencia documental por sobre la generaci√≥n de respuestas especulativas. El sistema no solo mejora la calidad de las respuestas positivas, sino que tambi√©n gestiona adecuadamente los casos de informaci√≥n incompleta.\n",
        "\n",
        "Finalmente, las respuestas asociadas a recomendaciones, l√≠mites y precauciones de uso mostraron que el pipeline es capaz de sintetizar lineamientos t√©cnicos a partir de m√∫ltiples fragmentos documentales, manteniendo coherencia y citabilidad. Los resultados confirman que la incorporaci√≥n de un pipeline de generaci√≥n aumentada aporta valor frente a un enfoque de recuperaci√≥n simple, mejorando la confiabilidad, explicabilidad y utilidad pr√°ctica del sistema RAG en un campo t√©cnico especializado como el de documentos cientificos"
      ],
      "metadata": {
        "id": "jUt-avOuDuuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2WJrRIYmEglP"
      }
    }
  ]
}