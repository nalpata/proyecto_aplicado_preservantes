"""
streamlit_app.py
Interfaz Streamlit para el sistema Preserv-RAG Hito 1.
Permite hacer consultas y ver mÃ©tricas baseline.
"""

import streamlit as st
import sys
from pathlib import Path

# Agregar src al path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from data_ingestion import PDFIngestion
from text_extraction import TextExtractor
from chunking import DocumentChunker
from metadata_extraction import MetadataExtractor
from vector_db import VectorDatabase
from retriever import SimpleRetriever
from benchmark import RAGBenchmark


# ConfiguraciÃ³n de Streamlit
st.set_page_config(
    page_title="Preserv-RAG - Hito 1 (Baseline)",
    page_icon="ğŸ§¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personalizado
st.markdown("""
    <style>
    .metric-box {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
        margin: 10px 0;
    }
    .retrieval-box {
        background-color: #e8f4f8;
        padding: 15px;
        border-left: 4px solid #1f77b4;
        margin: 10px 0;
    }
    </style>
""", unsafe_allow_html=True)

# TÃ­tulo
st.title("ğŸ§¬ Preserv-RAG - Sistema Baseline (Hito 1)")
st.markdown("**Pipeline RAG para recomendaciones de alternativas naturales a conservantes sintÃ©ticos**")

# Sidebar
with st.sidebar:
    st.header("âš™ï¸ ConfiguraciÃ³n")

    pdf_folder = st.text_input(
        "Carpeta con PDFs",
        value="data/pdfs",
        help="Ruta relativa o absoluta a la carpeta con archivos PDF"
    )

    st.divider()
    st.subheader("ğŸ” Filtrado de Contenido")

    filter_sections = st.checkbox(
        "Filtrar secciones de bajo valor",
        value=True,
        help="Remueve referencias, tablas, agradecimientos, etc."
    )

    if filter_sections:
        st.caption("Secciones a filtrar:")

        remove_references = st.checkbox(
            "ğŸ“š Referencias bibliogrÃ¡ficas",
            value=True,
            help="Elimina secciones de referencias y bibliografÃ­a"
        )

        remove_tables = st.checkbox(
            "ğŸ“Š Tablas",
            value=True,
            help="Elimina tablas detectadas en el texto"
        )

        remove_acknowledgments = st.checkbox(
            "ğŸ™ Agradecimientos",
            value=True,
            help="Elimina secciones de agradecimientos"
        )

        remove_appendix = st.checkbox(
            "ğŸ“ ApÃ©ndices",
            value=True,
            help="Elimina apÃ©ndices y anexos"
        )

        remove_headers_footers = st.checkbox(
            "ğŸ“„ Headers/Footers",
            value=True,
            help="Elimina encabezados y pies de pÃ¡gina"
        )
    else:
        remove_references = False
        remove_tables = False
        remove_acknowledgments = False
        remove_appendix = False
        remove_headers_footers = False

    st.divider()
    st.subheader("âœ‚ï¸ Chunking")

    chunk_size = st.slider(
        "TamaÃ±o de chunk (caracteres)",
        min_value=200,
        max_value=1500,
        value=500,
        step=100,
        help="TamaÃ±o aproximado de cada chunk (se respetan lÃ­mites de pÃ¡rrafo/oraciÃ³n)"
    )

    overlap = st.slider(
        "Solapamiento entre chunks",
        min_value=0,
        max_value=200,
        value=50,
        step=10,
        help="Caracteres de overlap inteligente entre chunks consecutivos"
    )

    st.divider()
    st.subheader("ğŸ” BÃºsqueda")

    n_results = st.slider(
        "NÃºmero de resultados a retornar",
        min_value=1,
        max_value=20,
        value=5
    )

    similarity_threshold = st.slider(
        "Umbral de similitud",
        min_value=0.0,
        max_value=1.0,
        value=0.3,
        step=0.05
    )

# Tabs principales
tab1, tab2, tab3, tab4 = st.tabs([
    "ğŸ”„ Pipeline",
    "ğŸ” BÃºsqueda",
    "ğŸ“Š MÃ©tricas Baseline",
    "ğŸ“‹ InformaciÃ³n"
])

# ============= TAB 1: PIPELINE =============
with tab1:
    st.header("Pipeline de Procesamiento")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("1ï¸âƒ£ Ingesta de PDFs")
        if st.button("Cargar PDFs", key="load_pdfs"):
            try:
                with st.spinner("Cargando PDFs..."):
                    ingestion = PDFIngestion(pdf_folder=pdf_folder)
                    documents = ingestion.load_pdfs()
                    stats = ingestion.get_stats()
                    
                    st.session_state['documents'] = documents
                    st.session_state['ingestion_stats'] = stats
                    
                    st.success(f"âœ“ {stats['total_pdfs']} PDFs cargados")
                    st.metric("Total de pÃ¡ginas", stats['total_pages'])
                    st.metric("Caracteres totales", f"{stats['total_characters']:,}")
            except Exception as e:
                st.error(f"âŒ Error: {str(e)}")
    
    with col2:
        if "ingestion_stats" in st.session_state:
            st.subheader("EstadÃ­sticas de Ingesta")
            stats = st.session_state['ingestion_stats']
            st.write(f"**PDFs cargados:** {stats['total_pdfs']}")
            st.write(f"**PÃ¡ginas totales:** {stats['total_pages']}")
            st.write(f"**Caracteres:** {stats['total_characters']:,}")
            st.write(f"**Prom. pÃ¡ginas/PDF:** {stats['avg_pages_per_pdf']:.2f}")
    
    st.divider()
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("2ï¸âƒ£ Limpieza y Filtrado de Texto")
        if st.button("Limpiar y Filtrar Texto", key="clean_text"):
            if "documents" not in st.session_state:
                st.warning("âš ï¸ Primero carga los PDFs")
            else:
                try:
                    with st.spinner("Limpiando y filtrando texto..."):
                        extractor = TextExtractor(
                            filter_sections=filter_sections,
                            remove_references=remove_references,
                            remove_acknowledgments=remove_acknowledgments,
                            remove_appendix=remove_appendix,
                            remove_tables=remove_tables,
                            remove_headers_footers=remove_headers_footers
                        )
                        cleaned_docs = extractor.process_documents(st.session_state['documents'])
                        stats = extractor.get_stats(cleaned_docs)

                        st.session_state['cleaned_documents'] = cleaned_docs
                        st.session_state['extraction_stats'] = stats

                        st.success("âœ“ Texto procesado")
                        st.metric("ReducciÃ³n de caracteres", f"{stats['reduction_percentage']}%")

                        if filter_sections:
                            st.info(f"ğŸ—‘ï¸ Secciones removidas: {stats['total_sections_removed']}")
                except Exception as e:
                    st.error(f"âŒ Error: {str(e)}")
    
    with col2:
        if "extraction_stats" in st.session_state:
            st.subheader("EstadÃ­sticas de Procesamiento")
            stats = st.session_state['extraction_stats']
            st.write(f"**Caracteres originales:** {stats['total_original_chars']:,}")
            st.write(f"**Caracteres finales:** {stats['total_cleaned_chars']:,}")
            st.write(f"**ReducciÃ³n:** {stats['reduction_percentage']}%")

            if stats.get('sections_filtered', False):
                st.write("---")
                st.write("**Secciones filtradas:**")
                st.write(f"- Referencias: {stats.get('total_references_removed', 0)}")
                st.write(f"- Tablas: {stats.get('total_tables_removed', 0)}")
                st.write(f"- Agradecimientos: {stats.get('total_acknowledgments_removed', 0)}")
                st.write(f"- ApÃ©ndices: {stats.get('total_appendix_removed', 0)}")
                st.write(f"- Headers/Footers: {stats.get('total_headers_footers_removed', 0)}")
    
    st.divider()
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("3ï¸âƒ£ Chunking")
        if st.button("Dividir en Chunks", key="chunk"):
            if "cleaned_documents" not in st.session_state:
                st.warning("âš ï¸ Primero limpia el texto")
            else:
                try:
                    with st.spinner("Dividiendo en chunks..."):
                        chunker = DocumentChunker(chunk_size=chunk_size, overlap=overlap)
                        chunks = chunker.chunk_documents(st.session_state['cleaned_documents'])
                        stats = chunker.get_stats(chunks)
                        
                        st.session_state['chunks'] = chunks
                        st.session_state['chunking_stats'] = stats
                        
                        st.success(f"âœ“ {stats['total_chunks']} chunks creados")
                except Exception as e:
                    st.error(f"âŒ Error: {str(e)}")
    
    with col2:
        if "chunking_stats" in st.session_state:
            st.subheader("EstadÃ­sticas de Chunking")
            stats = st.session_state['chunking_stats']
            st.write(f"**Total de chunks:** {stats['total_chunks']}")
            st.write(f"**Documentos Ãºnicos:** {stats['unique_documents']}")
            st.write(f"**TamaÃ±o promedio:** {stats['avg_chunk_size']} caracteres")
            st.write(f"**TamaÃ±o mÃ­n/mÃ¡x:** {stats['min_chunk_size']}/{stats['max_chunk_size']}")
            st.write(f"**Chunks/documento:** {stats['chunks_per_document']}")
            st.write(f"**PÃ¡rrafos/chunk:** {stats.get('avg_paragraphs_per_chunk', 'N/A')}")
    
    st.divider()
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("4ï¸âƒ£ ExtracciÃ³n de Metadata")
        if st.button("Extraer Metadata", key="extract_metadata"):
            if "chunks" not in st.session_state:
                st.warning("âš ï¸ Primero crea los chunks")
            else:
                try:
                    with st.spinner("Extrayendo metadata..."):
                        extractor = MetadataExtractor()
                        chunks_with_metadata = extractor.process_chunks(st.session_state['chunks'])
                        stats = extractor.get_stats(chunks_with_metadata)
                        
                        st.session_state['chunks_with_metadata'] = chunks_with_metadata
                        st.session_state['metadata_stats'] = stats
                        
                        st.success("âœ“ Metadata extraÃ­da")
                except Exception as e:
                    st.error(f"âŒ Error: {str(e)}")
    
    with col2:
        if "metadata_stats" in st.session_state:
            st.subheader("EstadÃ­sticas de Metadata")
            stats = st.session_state['metadata_stats']
            st.write(f"**Chunks con pH:** {stats['chunks_with_ph']}")
            st.write(f"**Chunks con aW:** {stats['chunks_with_aw']}")
            st.write(f"**Chunks con microorganismos:** {stats['chunks_with_microorganisms']}")
            st.write(f"**Chunks con conservantes:** {stats['chunks_with_conservants']}")
    
    st.divider()
    
    st.subheader("5ï¸âƒ£ VectorizaciÃ³n e IndexaciÃ³n")
    if st.button("Crear Base Vectorial", key="vectorize"):
        if "chunks_with_metadata" not in st.session_state:
            st.warning("âš ï¸ Primero extrae la metadata")
        else:
            try:
                with st.spinner("Creando base vectorial (puede tomar tiempo)..."):
                    vdb = VectorDatabase(db_path="data/chroma_db")
                    vdb.add_chunks(st.session_state['chunks_with_metadata'])
                    stats = vdb.get_collection_stats()
                    
                    st.session_state['vector_db'] = vdb
                    st.session_state['vdb_stats'] = stats
                    
                    st.success("âœ“ Base vectorial creada")
                    st.info(f"ğŸ“Š {stats['total_chunks']} chunks indexados con {stats['model_used']}")
            except Exception as e:
                st.error(f"âŒ Error: {str(e)}")


# ============= TAB 2: BÃšSQUEDA =============
with tab2:
    st.header("ğŸ” BÃºsqueda Vectorial")
    
    if "vector_db" not in st.session_state:
        st.warning("âš ï¸ Primero completa el pipeline en la pestaÃ±a 'Pipeline'")
    else:
        query = st.text_area(
            "Ingresa tu consulta",
            placeholder="Ej: Â¿QuÃ© alternativa natural puedo usar para reemplazar benzoato en una salsa con pH 4.2?",
            height=100
        )
        
        if st.button("Buscar", key="search"):
            if query.strip():
                with st.spinner("Buscando..."):
                    retriever = SimpleRetriever(st.session_state['vector_db'])
                    results = retriever.retrieve(query, n_results=n_results)
                    
                    if results:
                        st.success(f"âœ“ {len(results)} resultados encontrados")
                        
                        for i, result in enumerate(results, 1):
                            with st.container():
                                col1, col2 = st.columns([3, 1])
                                
                                with col1:
                                    st.markdown(f"### Resultado {i}")
                                    st.write(f"**Similitud:** {result['similarity_score']:.4f}")
                                    st.write(f"**Fuente:** {result['metadata'].get('source_file', 'Unknown')}")
                                    st.write(f"**Chunk ID:** `{result['id']}`")
                                
                                with col2:
                                    if result['similarity_score'] >= similarity_threshold:
                                        st.success("âœ“ Sobre threshold")
                                    else:
                                        st.warning("âš ï¸ Bajo threshold")
                                
                                st.markdown("**Contenido:**")
                                st.text(result['content'][:500] + "..." if len(result['content']) > 500 else result['content'])
                                
                                st.divider()
                    else:
                        st.info("No se encontraron resultados relevantes")
            else:
                st.warning("Por favor ingresa una consulta")


# ============= TAB 3: MÃ‰TRICAS BASELINE =============
with tab3:
    st.header("ğŸ“Š MÃ©tricas del Sistema Baseline")
    
    if "vector_db" not in st.session_state:
        st.warning("âš ï¸ Primero completa el pipeline")
    else:
        st.subheader("EstadÃ­sticas de la Base de Datos")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Total de chunks", st.session_state['vdb_stats']['total_chunks'])
        with col2:
            st.metric("Modelo de embeddings", st.session_state['vdb_stats']['model_used'])
        with col3:
            st.metric("Ruta de BD", st.session_state['vdb_db_path'] if "vdb_db_path" in st.session_state else "data/chroma_db")
        
        st.divider()
        
        st.subheader("EvaluaciÃ³n con Queries de Prueba")
        
        # Queries de prueba sugeridas
        default_queries = [
            ("Benzoato en pH 4.2 contra levaduras", ["chunk_id_1", "chunk_id_2"]),
            ("Alternativas naturales a sorbato", ["chunk_id_3"]),
            ("Nisina y extractos de plantas", ["chunk_id_4", "chunk_id_5"]),
        ]
        
        st.info("ğŸ’¡ Agrega queries de prueba para evaluar el sistema")
        
        col1, col2 = st.columns(2)
        with col1:
            query_input = st.text_input("Ingresa una query de prueba")
        with col2:
            relevant_input = st.text_input("Chunk IDs relevantes (separados por comas)")
        
        if st.button("Agregar Query de Prueba"):
            if query_input and relevant_input:
                if "test_queries" not in st.session_state:
                    st.session_state['test_queries'] = []
                
                relevant_ids = [id.strip() for id in relevant_input.split(",")]
                st.session_state['test_queries'].append((query_input, relevant_ids))
                st.success("âœ“ Query agregada")
        
        if "test_queries" in st.session_state and st.session_state['test_queries']:
            if st.button("Ejecutar Benchmark"):
                with st.spinner("Ejecutando evaluaciÃ³n..."):
                    retriever = SimpleRetriever(st.session_state['vector_db'])
                    benchmark = RAGBenchmark(retriever)
                    
                    aggregated = benchmark.evaluate_multiple_queries(st.session_state['test_queries'])
                    
                    st.session_state['benchmark_results'] = aggregated
                    st.success("âœ“ EvaluaciÃ³n completada")
        
        if "benchmark_results" in st.session_state:
            st.subheader("Resultados del Benchmark")
            
            results = st.session_state['benchmark_results']
            
            # MÃ©tricas principales
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Total de queries", results['total_queries'])
            with col2:
                st.metric("Precision@5", f"{results['avg_precision_at_5']:.4f}")
            with col3:
                st.metric("Recall@5", f"{results['avg_recall_at_5']:.4f}")
            with col4:
                st.metric("MRR", f"{results['avg_mrr']:.4f}")
            
            # MÃ©tricas adicionales
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("NDCG@5", f"{results['avg_ndcg_at_5']:.4f}")
            with col2:
                st.metric("NDCG@10", f"{results['avg_ndcg_at_10']:.4f}")
            with col3:
                st.metric("Similitud Promedio", f"{results['avg_similarity_score']:.4f}")
            
            st.markdown("**InterpretaciÃ³n:**")
            st.write("""
            - **Precision@K:** ProporciÃ³n de resultados relevantes en los top-K
            - **Recall@K:** ProporciÃ³n de documentos relevantes recuperados
            - **MRR:** PosiciÃ³n promedio del primer resultado relevante (1/rank)
            - **NDCG:** MÃ©trica que considera el ranking (relevancia ordenada)
            """)


# ============= TAB 4: INFORMACIÃ“N =============
with tab4:
    st.header("ğŸ“‹ InformaciÃ³n del Sistema")
    
    st.subheader("Arquitectura del Pipeline Hito 1")
    
    architecture = """
    ```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   PDFs (Ingesta)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ExtracciÃ³n de Texto     â”‚
    â”‚  (PyPDF2)                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Limpieza y NormalizaciÃ³n               â”‚
    â”‚  (regex, espacios, URLs, emails)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ†• DetecciÃ³n y Filtrado de Secciones    â”‚
    â”‚  - Referencias bibliogrÃ¡ficas            â”‚
    â”‚  - Tablas                                â”‚
    â”‚  - Agradecimientos/ApÃ©ndices             â”‚
    â”‚  - Headers/Footers                       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸ†• Chunking SemÃ¡ntico Mejorado           â”‚
    â”‚  - Respeta pÃ¡rrafos completos             â”‚
    â”‚  - Divide por oraciones (no abreviaciones)â”‚
    â”‚  - Overlap inteligente                    â”‚
    â”‚  - TamaÃ±o adaptativo                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ExtracciÃ³n de Metadata                   â”‚
    â”‚  (pH, aW, microorganismos, conservantes)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  VectorizaciÃ³n                       â”‚
    â”‚  (sentence-transformers)             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Chroma Vector Database   â”‚
    â”‚  (Almacenamiento)         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Retriever Simple      â”‚
    â”‚  (BÃºsqueda Vectorial)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Benchmark                    â”‚
    â”‚  (MÃ©tricas: P@K, R@K, MRR...)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```
    """
    
    st.markdown(architecture)
    
    st.subheader("Componentes del Sistema")
    
    components = {
        "data_ingestion.py": "Carga PDFs desde carpeta local",
        "text_extraction.py": "Limpia, normaliza y filtra secciones de bajo valor",
        "section_detector.py": "ğŸ†• Detecta y remueve referencias, tablas, agradecimientos",
        "chunking.py": "ğŸ†• Divide documentos con chunking semÃ¡ntico (pÃ¡rrafos/oraciones)",
        "metadata_extraction.py": "Extrae pH, aW, microorganismos, etc.",
        "vector_db.py": "Maneja base vectorial con Chroma",
        "retriever.py": "Recupera chunks similares",
        "benchmark.py": "Calcula mÃ©tricas de evaluaciÃ³n",
        "streamlit_app.py": "Interfaz de usuario",
    }
    
    for component, description in components.items():
        st.write(f"**{component}:** {description}")
    
    st.subheader("TecnologÃ­as Utilizadas")
    
    tech_table = """
    | Componente | LibrerÃ­a | PropÃ³sito |
    |---|---|---|
    | Lectura de PDFs | PyPDF2 | Extraer texto de PDFs |
    | Embeddings | sentence-transformers | Generar vectores de texto |
    | Vector DB | Chroma | Almacenar embeddings |
    | Interfaz | Streamlit | UI interactiva |
    | EvaluaciÃ³n | NumPy | CÃ¡lculo de mÃ©tricas |
    """
    
    st.markdown(tech_table)
    
    st.subheader("Flujo de Uso")
    
    st.markdown("""
    1. **Cargar PDFs** â†’ Selecciona la carpeta con archivos
    2. **Limpiar Texto** â†’ Normaliza el contenido
    3. **Crear Chunks** â†’ Divide en fragmentos
    4. **Extraer Metadata** â†’ Identifica pH, aW, etc.
    5. **Vectorizar** â†’ Crea embeddings e indexa
    6. **Buscar** â†’ Consulta la base vectorial
    7. **Evaluar** â†’ Mide rendimiento con mÃ©tricas
    """)
    
    st.divider()
    
    st.markdown("**VersiÃ³n:** Hito 1 (Baseline)")
    st.markdown("**Entrega:** 22 de Noviembre")
    st.markdown("**PrÃ³ximo:** Hito 2 - Mejoras avanzadas y generaciÃ³n con Claude")
